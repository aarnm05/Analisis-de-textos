View(matriz)
matriz = ala %>%
pivot_wider(id_cols = id,names_from= word,values_from= 1)
View(matriz)
#ya no tenemos el problema de los signos de exclamación a la mitad. Entonces no nos vamos a preocupar por esto
procesador_texto= function(x) {
otro  = str_trim(x) #le quitamos los espacios de mas
otro = otro[otro != ""] #quitamos las variables vacias basicamente
otro1 = cleantext(otro)
dummy = data.frame(texto = otro1, id = 1:length(otro1))
# Stopwords--------------------
beta = dummy %>%
unnest_tokens(word, texto) %>%  #en este paso separamos las palabras individualmente
anti_join(custom_stop)   #elimina las palabras que coincidan dentro del registro de stopwords
z = hunspell_stem(words = beta$word,dict = costa) #lematizacion
vacios = which(lapply(z, length) == 0)
beta = beta[-vacios,] #las palabras divididas le quito los terminos no reconocidos #en este caso un numero
z = z[-vacios]
beta$lema = lapply(z, `[[`, 1) %>% as.character()
beta
}
grafico_palabras = function(a,titulo,subtitulo){
ggplot(data = a,aes(x = reorder(word,n) ,y = n))+
geom_point(color = "blue")+
geom_segment(
aes(x=word,xend=word,y=0,yend=n),color="skyblue3")+
theme(axis.text.y = element_text(angle = 30,size=8,hjust=1))+
coord_flip()+
xlab("")+
theme_light()+
labs(title = titulo,
subtitle = subtitulo,
y = "Cantidad")+
theme(plot.title = element_text( hjust = 0.5),
plot.subtitle = element_text( hjust = 0.5),
panel.grid.major.y = element_blank(),
panel.border = element_blank(),
axis.ticks.y = element_blank())
}
library(dplyr)
library(tidytext)
library(hunspell)
library(readxl)
library(tidytext)
library(SnowballC)
library(stringr)
library(ggplot2)
library( tidyr)
library( tm )
library( ggraph)
library( igraph)
#setwd("~/GitHub/Analisis-de-textos")
hunspell::dictionary("costa/es_cr.dic")
#modifiquen esto a conveniencia, en mi git estan esos diccionarios. Fernando me preguntó por qué había tantos diccionarios. Es solo para probar
espa=dictionary("mex/es_MX.dic")
costa = dictionary("costa/es_cr.dic")
cast = dictionary("cast/es_ES.dic")
custom_stop= data_frame(word = tm::stopwords("spanish"), #stopwords
lexicon = "custom") #el nombre que le vamos a poner
dataset <-  suppressWarnings( foreign::read.spss("Actualidades.sav",to.data.frame = T) )#alternativamente en la carpeta está el csv, entonces pueden usar ese si no les funciona
#adaptado del siguiente codigo
#https://stackoverflow.com/questions/56026550/how-to-use-hunspell-package-to-suggest-correct-words-in-a-column-in-r
cleantext = function(x) {
sapply(1:length(x), function(y) {
bad = hunspell(x[y], dict = costa)[[1]]
good = unlist(lapply(hunspell_suggest(bad,dict = costa), `[[`, 1))
if (length(bad)) {
for (i in 1:length(bad)) {
x[y] <<- gsub(bad[i], good[i], x[y])
}
}
})
x
}
ala = procesador_texto(dataset$PR10B)
View(ala)
texto_procesado = procesador_texto(dataset$PR10B)
frec_simple = texto_procesado %>% #frecuencias de las palabras lematizadas
count(word, sort = T)
frec_lema = texto_procesado %>%
count(lema,sort = T )
colnames(frec_lema) = c("word","n")
frec_simple15 = frec_simple[1:15,]
frec_lema15 = frec_lema[1:15,]
grafico_palabras(frec_simple15,"Temas principales en las respuestas de los participantes","Grafico de frecuencia por palabra, sin lematizar")
grafico_palabras(frec_lema15,"Temas principales en las respuestas de los participantes","Grafico de frecuencia por palabra, lematizado")
View(texto_procesado)
matriz = texto_procesado %>%
group_by(id) %>%
count(lema,sort = T ) %>%
pivot_wider(id_cols = id,names_from= word,values_from= n)
matriz = texto_procesado %>%
group_by(id) %>%
count(lema,sort = T ) %>%
pivot_wider(id_cols = id,names_from= lema,values_from= n)
View(matriz)
palabras = dataset$PR10B
corrector.ortografia <- function(datos_texto, diccionario)
{
datos_corregidos <- data.frame(texto = datos_texto)
datos_corregidos$texto <- as.character(datos_corregidos$texto)
datos_corregidos$nuevo <- 0
for (i in 1:dim(datos_corregidos)[1])
{
respuesta <- datos_corregidos[i,"texto"]
incorrectas <- hunspell(respuesta, dict = diccionario) # Detectar palabras incorrectas
buenas <- NULL
if(length(unlist(incorrectas)) > 0)
{
for (k in 1:length(incorrectas[[1]]))
{
## hunspell_suggest retorna una lista con un vector de posibles palabras para hacer la corrección
buenas.sugeridas <- hunspell_suggest(incorrectas[[1]][k], dict = diccionario)
buenas[k] <- buenas.sugeridas[[1]][1] ## La primera es por lo general la más probable.
}
names(buenas) <- unlist(incorrectas)
datos_corregidos$nuevo[i] <- str_replace_all(respuesta, buenas)
}
else
{
datos_corregidos$nuevo[i] <- datos_corregidos[i,"texto"]
}
}
return(datos_corregidos)
}
corrector.lematizador <- function(datos_corregidos, diccionario)
{
datos_corregidos$nuevo <- as.character(datos_corregidos$nuevo)
datos_corregidos$nuevo_stem <- 0
for (i in 1:dim(datos_corregidos)[1])
{
respuesta <- datos_corregidos[i,"nuevo"]
palabras <- hunspell_parse(respuesta, format = "latex") ## Parsear el texto
proveniente <- hunspell_stem(unlist(palabras), dict = diccionario) ## Retorna una lista con la raíz  de cada palabra
texto <- NULL
if(length(unlist(proveniente)) > 0)
{
for (j in 1:length(proveniente))
{
palabra <- proveniente[[j]][1]
texto <- paste(texto, palabra, sep = " ")
}
datos_corregidos$nuevo_stem[i] <- texto
}
else
{
datos_corregidos$nuevo_stem[i] <- respuesta
}
}
return(datos_corregidos)
}
limpiar.texto <- function(datos){
## Transformar el texto a corpus
corpus <- VCorpus(VectorSource(datos$nuevo_stem))
## Remover los números
corpus <- tm_map(corpus, removeNumbers)
## Eliminar signos de puntuación
corpus <- tm_map(corpus, removePunctuation)
## Convertir a minúscula
corpus <- tm_map(corpus, content_transformer(tolower))
## Eliminar espacios en blanco de más
corpus <- tm_map(corpus, stripWhitespace)
## Establecer palabras vacías:
my_stopwords <- setdiff(stopwords("spanish"), c("también", "tampoco"))
## Eliminar palabras vacías
corpus <- tm_map(corpus, removeWords, my_stopwords)
corpus <- tm_map(corpus, stemDocument, language="spanish")
return(corpus)
}
palabras_bi = corrector.ortografia( palabras, costa)
palabras_bi = corrector.ortografia( palabras, costa)
palabras_bi = corrector.ortografia( palabras, costa)
palabras_bi2 = corrector.lematizador( palabras_bi, costa )
balabras_bifinal = limpiar.texto( palabras_bi2)
View(balabras_bifinal)
texto.df <- data.frame(respuesta = sapply(balabras_bifinal,
as.character),
stringsAsFactors = F)
View(texto.df)
View(matriz)
View(texto_procesado)
textito = texto_procesado[1:50,]
View(textito)
resultado = aggregate(lema~id, data = textito,paste, collapse=" ")
View(resultado)
lemas_bigramas = aggregate(lema~id, data = textito,paste, collapse=" ")
View(textito)
lema_bigrama = aggregate(lema~id, data = texto_procesado,paste, collapse=" ")
View(lema_bigrama)
View(palabras_bi2)
View(custom_stop)
which(custom_stop$word=="no")
custom_stop!="no"
custom_stop = custom_stop[custom_stop$word!="no",]
lema_bigrama = aggregate(lema~id, data = texto_procesado,paste, collapse=" ")
#crear bigramas
Our_bigram = texto.df %>%
unnest_tokens(bigram, respuesta, token = "ngrams", n = 2)
#contar los bigramas
Our_bigram_count = Our_bigram %>%
count(bigram, sort = TRUE)
View(Our_bigram_count)
#Separar bigramas para filtrarlos
bigramas_separados = Our_bigram_count %>%
separate( bigram, c("word1", "word2"), sep = " ")
View(bigramas_separados)
#bigramas con la palabra "no"
bigramas_con_no = bigramas_separados %>%
filter(word1 == "no") %>%
count(word1, word2, sort = TRUE)
View(bigramas_con_no)
bigrama_graf = bigramas_separados %>% filter( n > 5) %>%  graph_from_data_frame()#ese 5 es arbitrario
View(bigrama_graf)
View(texto.df)
balabras_bifinal = limpiar.texto( palabras_bi2)
View(lema_bigrama)
View(balabras_bifinal)
texto.df <- data.frame(respuesta = sapply(balabras_bifinal,
as.character),
stringsAsFactors = F)
View(texto.df)
Our_bigram2 = lema_bigrama %>%
unnest_tokens(bigram, respuesta, token = "ngrams", n = 2)
Our_bigram2 = lema_bigrama %>%
unnest_tokens(bigram, lema, token = "ngrams", n = 2)
View(Our_bigram)
View(Our_bigram2)
View(texto.df)
Our_bigram2 = lema_bigrama %>%
unnest_tokens(bigram, lema, token = "ngrams", n = 2)
#crear bigramas
Our_bigram = texto.df %>%
unnest_tokens(bigram, respuesta, token = "ngrams", n = 2)
View(Our_bigram)
View(lema_bigrama)
Our_bigram2 = lema_bigrama %>%
select(lema) %>%
unnest_tokens(bigram, lema, token = "ngrams", n = 2)
#grafico de la tesis, mas bonito
word_cors <- texto.df %>%
mutate(section = row_number()) %>%
unnest_tokens(word, respuesta,
token = "ngrams",
n = 1)  %>%
group_by(word) %>%
filter(n() >= 5) %>%
widyr::pairwise_cor(word, section,
sort = TRUE)
word_cors
word_cors2 <- lema_bigrama %>%
mutate(section = row_number()) %>%
unnest_tokens(word, respuesta,
token = "ngrams",
n = 1)  %>%
group_by(word) %>%
filter(n() >= 5) %>%
widyr::pairwise_cor(word, section,
sort = TRUE)
word_cors2 <- lema_bigrama %>%
mutate(section = row_number()) %>%
unnest_tokens(word, lema, token = "ngrams", n = 1)  %>%
group_by(word) %>%
filter(n() >= 5) %>%
widyr::pairwise_cor(word, section,
sort = TRUE)
word_cors2
word_cors2
word_cors
ggraph(bigrama_graf, layout = "fr") +
geom_edge_link() +
geom_node_point() +
geom_node_text(aes(label = name), vjust = 1, hjust = 1)
word_cors %>%
filter(correlation >= .25) %>%
graph_from_data_frame() %>%
ggraph(layout = "fr") +
geom_edge_link(aes(#edge_alpha = correlation,
edge_width = correlation,
colour = correlation)) +
geom_node_point(color = "#29e05d7a", size = 5) +
geom_node_text(aes(label = name), repel = TRUE) +
theme_void()
ggraph(bigrama_graf, layout = "fr") +
geom_edge_link() +
geom_node_point() +
geom_node_text(aes(label = name), vjust = 1, hjust = 1)
word_cors %>%
filter(correlation >= .25) %>%
graph_from_data_frame() %>%
ggraph(layout = "fr") +
geom_edge_link(aes(#edge_alpha = correlation,
edge_width = correlation,
colour = correlation)) +
geom_node_point(color = "#29e05d7a", size = 5) +
geom_node_text(aes(label = name), repel = TRUE) +
theme_void()
word_cors %>%
filter( abs(correlation ) >= .25) %>%
graph_from_data_frame() %>%
ggraph(layout = "fr") +
geom_edge_link(aes(#edge_alpha = correlation,
edge_width = correlation,
colour = correlation)) +
geom_node_point(color = "#29e05d7a", size = 5) +
geom_node_text(aes(label = name), repel = TRUE) +
theme_void()
ggraph(bigrama_graf, layout = "fr") +
geom_edge_link() +
geom_node_point() +
geom_node_text(aes(label = name), vjust = 1, hjust = 1)
word_cors %>%
filter(correlation >= .25) %>%
graph_from_data_frame() %>%
ggraph(layout = "fr") +
geom_edge_link(aes(#edge_alpha = correlation,
edge_width = correlation,
colour = correlation)) +
geom_node_point(color = "#29e05d7a", size = 5) +
geom_node_text(aes(label = name), repel = TRUE) +
theme_void()
word_cors %>%
filter( abs(correlation ) >= .25) %>%
graph_from_data_frame() %>%
ggraph(layout = "fr") +
geom_edge_link(aes(#edge_alpha = correlation,
edge_width = correlation,
colour = correlation)) +
geom_node_point(color = "#29e05d7a", size = 5) +
geom_node_text(aes(label = name), repel = TRUE) +
theme_void()
Our_bigram_count[1:15,] %>%
ggplot(aes(x = reorder(bigram,n) ,y = n))+
geom_point(color = "blue")+
geom_segment(
aes(x=bigram,xend=bigram,y=0,yend=n),color="skyblue3")+
theme(axis.text.y = element_text(angle = 30,size=8,hjust=1))+
coord_flip()+
xlab("")+
theme_light()+
labs(title = "Temas principales en las respuestas de los participantes",
subtitle = "Grafico de frecuencia por palabra, con bigramas",
y = "Cantidad")+
theme(plot.title = element_text( hjust = 0.5),
plot.subtitle = element_text( hjust = 0.5),
panel.grid.major.y = element_blank(),
panel.border = element_blank(),
axis.ticks.y = element_blank())
ggraph(bigrama_graf, layout = "fr") +
geom_edge_link() +
geom_node_point() +
geom_node_text(aes(label = name), vjust = 1, hjust = 1)
word_cors %>%
filter(correlation >= .25) %>%
graph_from_data_frame() %>%
ggraph(layout = "fr") +
geom_edge_link(aes(#edge_alpha = correlation,
edge_width = correlation,
colour = correlation)) +
geom_node_point(color = "#29e05d7a", size = 5) +
geom_node_text(aes(label = name), repel = TRUE) +
theme_void()
word_cors %>%
filter( abs(correlation ) >= .25) %>%
graph_from_data_frame() %>%
ggraph(layout = "fr") +
geom_edge_link(aes(#edge_alpha = correlation,
edge_width = correlation,
colour = correlation)) +
geom_node_point(color = "#29e05d7a", size = 5) +
geom_node_text(aes(label = name), repel = TRUE) +
theme_void()
Our_bigram_count[1:15,] %>%
ggplot(aes(x = reorder(bigram,n) ,y = n))+
geom_point(color = "blue")+
geom_segment(
aes(x=bigram,xend=bigram,y=0,yend=n),color="skyblue3")+
theme(axis.text.y = element_text(angle = 30,size=8,hjust=1))+
coord_flip()+
xlab("")+
theme_light()+
labs(title = "Temas principales en las respuestas de los participantes",
subtitle = "Grafico de frecuencia por palabra, con bigramas",
y = "Cantidad")+
theme(plot.title = element_text( hjust = 0.5),
plot.subtitle = element_text( hjust = 0.5),
panel.grid.major.y = element_blank(),
panel.border = element_blank(),
axis.ticks.y = element_blank())
Our_bigram_count %>%
Our_bigram_count
graf_bigrama = Our_bigram_count[1:15,]
View(graf_bigrama)
colnames(graf_bigrama) = c("word","n")
grafico_palabras(a = graf_bigrama,
titulo = "Temas principales en las respuestas de los participantes",
subtitulo = "Grafico de frecuencia por palabra, con bigramas")
Our_bigram_count[1:15,] %>%
ggplot(aes(x = reorder(bigram,n) ,y = n))+
geom_point(color = "blue")+
geom_segment(
aes(x=bigram,xend=bigram,y=0,yend=n),color="skyblue3")+
theme(axis.text.y = element_text(angle = 30,size=8,hjust=1))+
coord_flip()+
xlab("")+
theme_light()+
labs(title = "Temas principales en las respuestas de los participantes",
subtitle = "Grafico de frecuencia por palabra, con bigramas",
y = "Cantidad")+
theme(plot.title = element_text( hjust = 0.5),
plot.subtitle = element_text( hjust = 0.5),
panel.grid.major.y = element_blank(),
panel.border = element_blank(),
axis.ticks.y = element_blank())
graf_bigrama = Our_bigram_count[1:15,]
colnames(graf_bigrama) = c("word","n")
grafico_palabras(a = graf_bigrama,
titulo = "Temas principales en las respuestas de los participantes",
subtitulo = "Grafico de frecuencia por palabra, con bigramas")
library(dplyr)
library(tidytext)
library(hunspell)
library(tidytext)
library(stringr)
library(ggplot2)
library( tidyr)
library(ggraph)
library(igraph)
#setwd("~/GitHub/Analisis-de-textos")
hunspell::dictionary("costa/es_cr.dic")
espa=dictionary("mex/es_MX.dic")
costa = dictionary("costa/es_cr.dic")
cast = dictionary("cast/es_ES.dic")
custom_stop= data_frame(word = tm::stopwords("spanish"), #stopwords
lexicon = "custom") #el nombre que le vamos a poner
custom_stop = custom_stop[custom_stop$word!="no",]
dataset <-  suppressWarnings( foreign::read.spss("Actualidades.sav",to.data.frame = T) )
#adaptado del siguiente codigo
#https://stackoverflow.com/questions/56026550/how-to-use-hunspell-package-to-suggest-correct-words-in-a-column-in-r
cleantext = function(x) {
sapply(1:length(x), function(y) {
bad = hunspell(x[y], dict = costa)[[1]]
good = unlist(lapply(hunspell_suggest(bad,dict = costa), `[[`, 1))
if (length(bad)) {
for (i in 1:length(bad)) {
x[y] <<- gsub(bad[i], good[i], x[y])
}
}
})
x
}
procesador_texto= function(x) {
otro  = str_trim(x) #le quitamos los espacios de mas
otro = otro[otro != ""] #quitamos las variables vacias basicamente
otro1 = cleantext(otro)
dummy = data.frame(texto = otro1, id = 1:length(otro1))
# Stopwords--------------------
beta = dummy %>%
unnest_tokens(word, texto) %>%  #en este paso separamos las palabras individualmente
anti_join(custom_stop)   #elimina las palabras que coincidan dentro del registro de stopwords
z = hunspell_stem(words = beta$word,dict = costa) #lematizacion
vacios = which(lapply(z, length) == 0)
beta = beta[-vacios,] #las palabras divididas le quito los terminos no reconocidos #en este caso un numero
z = z[-vacios]
beta$lema = lapply(z, `[[`, 1) %>% as.character()
beta
}
grafico_palabras = function(a,titulo,subtitulo){
ggplot(data = a,aes(x = reorder(word,n) ,y = n))+
geom_point(color = "blue")+
geom_segment(
aes(x=word,xend=word,y=0,yend=n),color="skyblue3")+
theme(axis.text.y = element_text(angle = 30,size=8,hjust=1))+
coord_flip()+
xlab("")+
theme_light()+
labs(title = titulo,
subtitle = subtitulo,
y = "Cantidad")+
theme(plot.title = element_text( hjust = 0.5),
plot.subtitle = element_text( hjust = 0.5),
panel.grid.major.y = element_blank(),
panel.border = element_blank(),
axis.ticks.y = element_blank())
}
texto_procesado = procesador_texto(dataset$PR10B)
frec_lema = texto_procesado %>%
count(lema,sort = T )
colnames(frec_lema) = c("word","n")
frec_lema15 = frec_lema[1:15,]
grafico_palabras(frec_lema15,"Temas principales en las respuestas de los participantes","Grafico de frecuencia por palabra, lematizado")
lema_bigrama = aggregate(lema~id, data = texto_procesado,paste, collapse=" ") #oraciones sin stopwords y con gramatica corregida
Our_bigram = lema_bigrama %>%
select(lema) %>%
unnest_tokens(bigram, lema, token = "ngrams", n = 2)
#contar los bigramas
Our_bigram_count = Our_bigram %>%
count(bigram, sort = TRUE)
#Separar bigramas para filtrarlos
bigramas_separados = Our_bigram_count %>%
separate( bigram, c("word1", "word2"), sep = " ")
#bigramas con la palabra "no"
bigramas_con_no = bigramas_separados %>%
filter(word1 == "no") %>%
count(word1, word2, sort = TRUE)
word_cors2 <- lema_bigrama %>%
mutate(section = row_number()) %>%
unnest_tokens(word, lema, token = "ngrams", n = 1)  %>%
group_by(word) %>%
filter(n() >= 5) %>%
widyr::pairwise_cor(word, section,
sort = TRUE)
palabras_bi = corrector.ortografia( palabras, costa)
