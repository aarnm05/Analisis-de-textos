espa=dictionary("C:/Users/emirr/Documents/Sortone/OneDrive/Git/Analisis-de-textos/mex/")
espa=dictionary("C:/Users/emirr/Documents/Sortone/OneDrive/Git/Analisis-de-textos/mex/es_MX.dic")
espa
hunspell(dataset$P1COM[1],dict = espa)
costa = dictionary("C:/Users/emirr/Documents/Sortone/OneDrive/Git/Analisis-de-textos/costa/es_cr.dic")
hunspell(dataset$P1COM[1],dict = costa)
hunspell(dataset$P1COM,dict = costa)
k = hunspell(dataset$P1COM,dict = costa)
View(k)
b = hunspell_suggest(k,dict=costa)
View(k)
k[[]]
k[]
b = hunspell_suggest(k[],dict=costa)
k[[1]]
hunspell_suggest(k[[1]],dict=costa)
hunspell_suggest(k[[1]],dict= espa)
cast = dictionary("C:/Users/emirr/Documents/Sortone/OneDrive/Git/Analisis-de-textos/costa/es_ES.dic")
cast = dictionary("C:/Users/emirr/Documents/Sortone/OneDrive/Git/Analisis-de-textos/cast/es_ES.dic")
hunspell_suggest(k[[1]],dict= cast)
hunspell_suggest(k[[1]],dict= costa)
k
k = hunspell(dataset$P1COM,dict = costa)
hunspell_suggest(k[[1]],dict= costa)
hunspell_suggest(k[[1]],dict= costa)[[1]][1]
hunspell_suggest(k[[1]],dict= costa)
k
k
View(k)
k[[1]]
hunspell_suggest(k[[1]],dict= costa)
hunspell_suggest(k[[1]],dict= costa)[[2]]
k[[1]]
k[[1]]
k
k[[1]] %>%
as.data.frame()
r = k[[1]] %>%
as.data.frame()
View(r)
r = k %>%
as.data.frame()
r = k %>%
as.data.frame()
View(r)
View(k)
cleantext = function(x) {
sapply(1:length(x), function(y) {
bad = hunspell(x[y], dict = costa)[[1]]
good = unlist(lapply(hunspell_suggest(bad), `[[`, 1))
if (length(bad)) {
for (i in 1:length(bad)) {
x[y] <<- gsub(bad[i], good[i], x[y])
}
}
})
x
}
cleantext(dataset$P1COM)
dataset$P1COM[1]
cleantext(dataset$P1COM[1])
cleantext = function(x) {
sapply(1:length(x), function(y) {
bad = hunspell(x[y], dict = costa)[[1]]
good = unlist(lapply(hunspell_suggest(bad,dict = costa), `[[`, 1))
if (length(bad)) {
for (i in 1:length(bad)) {
x[y] <<- gsub(bad[i], good[i], x[y])
}
}
})
x
}
cleantext(dataset$P1COM[1])
cleantext(dataset$P1COM[2])
library(dplyr)
library(tidytext)
library(hunspell)
library(readxl)
library(tidytext)
library(SnowballC)
library(stringr)
hunspell::list_dictionaries()
#modifiquen esto a conveniencia, en mi git estan esos diccionarios
espa=dictionary("C:/Users/emirr/Documents/Sortone/OneDrive/Git/Analisis-de-textos/mex/es_MX.dic")
costa = dictionary("C:/Users/emirr/Documents/Sortone/OneDrive/Git/Analisis-de-textos/costa/es_cr.dic")
cast = dictionary("C:/Users/emirr/Documents/Sortone/OneDrive/Git/Analisis-de-textos/cast/es_ES.dic")
dataset <- foreign::read.spss("PILOTO.sav",to.data.frame = T) #mi dataset
otro  = str_trim(dataset$PR10B) #le quitamos los espacios de mas
otro = otro[otro!=""] #quitamos las variables vacias basicamente
otro #podemos ver que hay problemas por el !.
hunspell(otro,dict = costa) #prueba de como funciona hunspell
#adaptado del siguiente codigo
#https://stackoverflow.com/questions/56026550/how-to-use-hunspell-package-to-suggest-correct-words-in-a-column-in-r
cleantext = function(x) {
sapply(1:length(x), function(y) {
bad = hunspell(x[y], dict = costa)[[1]]
good = unlist(lapply(hunspell_suggest(bad,dict = costa), `[[`, 1))
if (length(bad)) {
for (i in 1:length(bad)) {
x[y] <<- gsub(bad[i], good[i], x[y])
}
}
})
x
}
#plan de contingencia
#ejemplo
ar= str_replace("nataci!n","\\!",replacement = "e") #basicamente vamos a hacer esto, reemplazar el signo de exclamacion por una vocal
ar
cleantext(ar)
remove(ar)
#fin del ejemplo
#hacemos esto porque la funcion toma un signo de exclamacion como el fin de una oracion (de forma correcta segun el español); consecuentemente lo va a tomar como si fuera dos palabras.
otro1 = str_replace_all(otro,"\\!", replacement = "e") #puede que no les funcione dependiendo de su version de R. En ese caso tendrían que borrarle un backslash.
otro1 = cleantext(otro1)
#fin de plan de contingencia
ana = data.frame(texto = otro1, id = 1:length(otro1))
custom_stop= data_frame(word = tm::stopwords("spanish"), #podemos hacerlo así
lexicon = "custom")
gato = ana %>%
unnest_tokens(word, texto) %>%  #en este paso separamos las palabras individualmente
anti_join(custom_stop)   #elimina las palabras que coincidan dentro del registro de stopwords
#gato tiene las palabras sin lematizar separadas, gato sirve para casos donde no queremos lematizar
frec_persona = gato %>% #frecuencias de las palabras sin lematizar
group_by(id) %>% #si quisieramos por persona para controlar si alguien dijo muchas veces una palabra
count(word,sort = T)
frec_simple=gato %>% #frecuencias de las palabras sin lematizar
count(word,sort = T)
frec_res = frec_simple[1:15,]
library(ggplot2)
ggplot(data = frec_res,aes(x= reorder(word,n) ,y =n))+
geom_point(color="blue")+
geom_segment(
aes(x=word,xend=word,y=0,yend=n),color="skyblue3")+
theme(axis.text.y = element_text(angle = 30,size=8,hjust=1))+
coord_flip()+
xlab("")+
theme_light()
#en este caso tenemos un grafico muy aburrido porque todas tienen la misma frecuencia porque el tamaño es muy pequeño
gato
data = data_frame(txt = sample(terms, 100, replace = TRUE))
terms <- c("emails are nice", "emailing is fun", "computer freaks", "broken modem")
data = data_frame(txt = sample(terms, 100, replace = TRUE))
data %>%
unnest_tokens(word, txt) %>%
mutate(word = wordStem(word))
terms <- c("emails are nice", "emailing is fun", "computer freaks", "broken modem")
data = data_frame(txt = terms)
data
data %>%
unnest_tokens(word, txt) %>%
mutate(word = wordStem(word))
ara = data %>%
unnest_tokens(word, txt) %>%
mutate(word = wordStem(word))
View(ara)
getStemLanguages()
terms <- c("salgo a correr","comemos juntos","corremos","salimos")
data = data_frame(txt = terms)
ara = data %>%
unnest_tokens(word, txt) %>%
mutate(word = wordStem(word,language = "spanish"))
View(ara)
hunspell::list_dictionaries()
#modifiquen esto a conveniencia, en mi git estan esos diccionarios
espa=dictionary("C:/Users/emirr/Documents/Sortone/OneDrive/Git/Analisis-de-textos/mex/es_MX.dic")
costa = dictionary("C:/Users/emirr/Documents/Sortone/OneDrive/Git/Analisis-de-textos/costa/es_cr.dic")
cast = dictionary("C:/Users/emirr/Documents/Sortone/OneDrive/Git/Analisis-de-textos/cast/es_ES.dic")
hunspell::list_dictionaries()
hunspell::list_dictionaries()
hunspell::dictionary("C:/Users/emirr/Documents/Sortone/OneDrive/Git/Analisis-de-textos/costa/es_cr.dic")
hunspell::list_dictionaries()
hunspell::dicpath("C:/Users/emirr/Documents/Sortone/OneDrive/Git/Analisis-de-textos/costa/es_cr.dic")
costa
espa
hunspell_parse(data,format = "latex")
terms <- c("salgo a correr","comemos juntos","corremos","salimos")
hunspell_parse(terms,format = "latex")
hunspell_stem(words = terms, dict = costa)
terms <- c("salgo a correr","comemos juntos","corremos","salimos")
hunspell_stem(words = terms, dict = costa)
hunspell_stem(words = gato$word, dict = costa)
gato
hunspell_stem(words = gato$word[1:5], dict = costa)
gato$word[1:5]
hunspell_stem(words = gato$word[1:5], dict = costa)
gato$word[1:5]
hunspell_stem(words = gato$word[1:10], dict = costa)
hunspell_stem(words = gato$word[1:10], dict = costa)
gato$word[1:10]
ara = hunspell_stem(words = gato$word[1:10], dict = costa) %>%
unlist()
ara = hunspell_stem(words = gato$word[1:10], dict = costa)
View(ara)
ara = hunspell_stem(words = gato$word[1:10], dict = costa) %>%
data.frame()
View(ana)
View(ara)
ara = hunspell_stem(words = gato$word[1:10], dict = costa) %>%
unlist()
ara
ara = hunspell_stem(words = gato$word[1:10], dict = costa)
View(ara)
select(gato,"word") %>%  #necesita un poco de trabajo todavia
SnowballC::wordStem(language = "spanish")
SnowballC::wordStem(language = "spanish",words = gato$word[1:10])
hunspell_stem(words = gato$word[1:10], dict = costa)
hunspell_stem(words = gato$word[6],dict = costa)
hunspell_stem(words = gato$word[6],dict = costa)[[1]]
hunspell_stem(words = gato$word[6],dict = costa)[[1]][1]
lema = gato %>%
mutate(lemas =hunspell_stem(words = word,dict = costa)[[1]][1] )
View(lema)
lema = gato %>%
mutate(lemas =hunspell_stem(words = word,dict = costa)[[]][1] )
lema = gato %>%
mutate(lemas =hunspell_stem(words = word,dict = costa) )
View(lema)
hunspell_stem(words = gato$word,dict = costa)[1]
lema = gato %>%
mutate(lemas =hunspell_stem(words = word,dict = costa)[1] )
lemas =hunspell_stem(words = word,dict = costa)[[
hunspell_stem(words = gato$word[1:4],dict = costa)[[
hunspell_stem(words = gato$word[1:4],dict = costa)[[ 1
lapply ( hunspell_stem(words = gato$word[1:4],dict = costa), "[[",1)
x= hunspell_stem(words = gato$word[1:4],dict = costa)
lapply(x, `[[`, 1)
A = lapply(x, `[[`, 1)
View(A)
A = lapply(x, `[[`, 1) %>%
data.frame()
View(A)
A = lapply(x, `[[`, 1) %>%
data.frame() %>% t()
View(A)
A = lapply(hunspell_stem(words = gato$word[1:4],dict = costa), `[[`, 1) %>%
data.frame() %>% t()
library(dplyr)
library(tidytext)
library(hunspell)
library(readxl)
library(tidytext)
library(SnowballC)
library(stringr)
hunspell::list_dictionaries()
hunspell::dictionary("C:/Users/emirr/Documents/Sortone/OneDrive/Git/Analisis-de-textos/costa/es_cr.dic")
#modifiquen esto a conveniencia, en mi git estan esos diccionarios
espa=dictionary("C:/Users/emirr/Documents/Sortone/OneDrive/Git/Analisis-de-textos/mex/es_MX.dic")
costa = dictionary("C:/Users/emirr/Documents/Sortone/OneDrive/Git/Analisis-de-textos/costa/es_cr.dic")
cast = dictionary("C:/Users/emirr/Documents/Sortone/OneDrive/Git/Analisis-de-textos/cast/es_ES.dic")
dataset <- foreign::read.spss("PILOTO.sav",to.data.frame = T) #mi dataset
otro  = str_trim(dataset$PR10B) #le quitamos los espacios de mas
otro = otro[otro!=""] #quitamos las variables vacias basicamente
otro #podemos ver que hay problemas por el !.
hunspell(otro,dict = costa) #prueba de como funciona hunspell
#adaptado del siguiente codigo
#https://stackoverflow.com/questions/56026550/how-to-use-hunspell-package-to-suggest-correct-words-in-a-column-in-r
cleantext = function(x) {
sapply(1:length(x), function(y) {
bad = hunspell(x[y], dict = costa)[[1]]
good = unlist(lapply(hunspell_suggest(bad,dict = costa), `[[`, 1))
if (length(bad)) {
for (i in 1:length(bad)) {
x[y] <<- gsub(bad[i], good[i], x[y])
}
}
})
x
}
#plan de contingencia
#ejemplo
ar= str_replace("nataci!n","\\!",replacement = "e") #basicamente vamos a hacer esto, reemplazar el signo de exclamacion por una vocal
ar
cleantext(ar)
remove(ar)
#fin del ejemplo
#hacemos esto porque la funcion toma un signo de exclamacion como el fin de una oracion (de forma correcta segun el español); consecuentemente lo va a tomar como si fuera dos palabras.
otro1 = str_replace_all(otro,"\\!", replacement = "e") #puede que no les funcione dependiendo de su version de R. En ese caso tendrían que borrarle un backslash.
otro1 = cleantext(otro1)
#fin de plan de contingencia
ana = data.frame(texto = otro1, id = 1:length(otro1))
custom_stop= data_frame(word = tm::stopwords("spanish"), #podemos hacerlo así
lexicon = "custom")
gato = ana %>%
unnest_tokens(word, texto) %>%  #en este paso separamos las palabras individualmente
anti_join(custom_stop)   #elimina las palabras que coincidan dentro del registro de stopwords
#gato tiene las palabras sin lematizar separadas, gato sirve para casos donde no queremos lematizar
frec_persona = gato %>% #frecuencias de las palabras sin lematizar
group_by(id) %>% #si quisieramos por persona para controlar si alguien dijo muchas veces una palabra
count(word,sort = T)
frec_simple=gato %>% #frecuencias de las palabras sin lematizar
count(word,sort = T)
frec_res = frec_simple[1:15,]
library(ggplot2)
ggplot(data = frec_res,aes(x= reorder(word,n) ,y =n))+
geom_point(color="blue")+
geom_segment(
aes(x=word,xend=word,y=0,yend=n),color="skyblue3")+
theme(axis.text.y = element_text(angle = 30,size=8,hjust=1))+
coord_flip()+
xlab("")+
theme_light()
#en este caso tenemos un grafico muy aburrido porque todas tienen la misma frecuencia porque el tamaño es muy pequeño
A = x %>% lapply( `[[`, 1) %>%
data.frame() %>% t()
SnowballC::wordStem(language = "spanish",words = gato$word[1:10]) #no es muy util, toma muy literal lo de la raiz
hunspell_stem(words = gato$word[1:10], dict = costa)
lema = gato %>%
mutate(lemas =hunspell_stem(words = word,dict = costa) )
x= hunspell_stem(words = gato$word[1:4],dict = costa)
A = x %>% lapply( `[[`, 1) %>%
data.frame() %>% t()
x %>% lapply( `[[`, 1)
lema = gato %>%
mutate(lemas = hunspell_stem(words = word,dict = costa) %>%
lapply( `[[`, 1)
)
gato$lemas = hunspell_stem(words = word,dict = costa) %>%
lapply( `[[`, 1)
gato$lemas = hunspell_stem(words = gato$word,dict = costa) %>%
lapply( `[[`, 1)
hunspell_stem(words = gato$word,dict = costa) %>%
lapply( `[[`, 1)
hunspell_stem(words = gato$word,dict = costa)
x = hunspell_stem(words = gato$word,dict = costa)
#%>%
y = lapply(x, `[[`, 1)
x
View(x)
View(gato)
gato$word[129]
View(dataset)
View(ana)
#%>%
y = lapply(x[1:100], `[[`, 1)
#%>%
y = lapply(x, `[[`, 1)
View(custom_stop)
View(ana)
View(custom_stop)
View(dataset)
gato[49,]
gato[119,]
gato$word[129]
gato[129,]
gato[129]
gato[129,]
gato[129,]
is.numeric(gato[129,]$word)
View(x)
x[[129]]
gato[129,]$word
gato = gato[-129,] #manualmente elimino un número que algun digitador agrego por error en una pregunta abierta. Gracias. Se puede verificar haciendo un ciclo o un apply para ver
length(x)
lapply(x, `[[`, length)
lapply(x, length, 1)
lapply(x, length(), 1)
lapply(x, dim)
lapply(x, length)
which(lapply(x, length) = 0)
which(lapply(x, length) == 0)
x[[129]]
z = x[[-129]]
z = x[129]
z = x[-129]
View(z)
library(dplyr)
library(tidytext)
library(hunspell)
library(readxl)
library(tidytext)
library(SnowballC)
library(stringr)
hunspell::list_dictionaries()
hunspell::dictionary("C:/Users/emirr/Documents/Sortone/OneDrive/Git/Analisis-de-textos/costa/es_cr.dic")
#modifiquen esto a conveniencia, en mi git estan esos diccionarios
espa=dictionary("C:/Users/emirr/Documents/Sortone/OneDrive/Git/Analisis-de-textos/mex/es_MX.dic")
costa = dictionary("C:/Users/emirr/Documents/Sortone/OneDrive/Git/Analisis-de-textos/costa/es_cr.dic")
cast = dictionary("C:/Users/emirr/Documents/Sortone/OneDrive/Git/Analisis-de-textos/cast/es_ES.dic")
dataset <- foreign::read.spss("PILOTO.sav",to.data.frame = T) #mi dataset
otro  = str_trim(dataset$PR10B) #le quitamos los espacios de mas
otro = otro[otro!=""] #quitamos las variables vacias basicamente
otro #podemos ver que hay problemas por el !.
hunspell(otro,dict = costa) #prueba de como funciona hunspell
#adaptado del siguiente codigo
#https://stackoverflow.com/questions/56026550/how-to-use-hunspell-package-to-suggest-correct-words-in-a-column-in-r
cleantext = function(x) {
sapply(1:length(x), function(y) {
bad = hunspell(x[y], dict = costa)[[1]]
good = unlist(lapply(hunspell_suggest(bad,dict = costa), `[[`, 1))
if (length(bad)) {
for (i in 1:length(bad)) {
x[y] <<- gsub(bad[i], good[i], x[y])
}
}
})
x
}
#plan de contingencia
#ejemplo
ar= str_replace("nataci!n","\\!",replacement = "e") #basicamente vamos a hacer esto, reemplazar el signo de exclamacion por una vocal
ar
cleantext(ar)
remove(ar)
#fin del ejemplo
#hacemos esto porque la funcion toma un signo de exclamacion como el fin de una oracion (de forma correcta segun el español); consecuentemente lo va a tomar como si fuera dos palabras.
otro1 = str_replace_all(otro,"\\!", replacement = "e") #puede que no les funcione dependiendo de su version de R. En ese caso tendrían que borrarle un backslash.
otro1 = cleantext(otro1)
#fin de plan de contingencia
ana = data.frame(texto = otro1, id = 1:length(otro1))
custom_stop= data_frame(word = tm::stopwords("spanish"), #podemos hacerlo así
lexicon = "custom")
gato = ana %>%
unnest_tokens(word, texto) %>%  #en este paso separamos las palabras individualmente
anti_join(custom_stop)   #elimina las palabras que coincidan dentro del registro de stopwords
#gato tiene las palabras sin lematizar separadas, gato sirve para casos donde no queremos lematizar
frec_persona = gato %>% #frecuencias de las palabras sin lematizar
group_by(id) %>% #si quisieramos por persona para controlar si alguien dijo muchas veces una palabra
count(word,sort = T)
frec_simple=gato %>% #frecuencias de las palabras sin lematizar
count(word,sort = T)
frec_res = frec_simple[1:15,]
library(ggplot2)
ggplot(data = frec_res,aes(x= reorder(word,n) ,y =n))+
geom_point(color="blue")+
geom_segment(
aes(x=word,xend=word,y=0,yend=n),color="skyblue3")+
theme(axis.text.y = element_text(angle = 30,size=8,hjust=1))+
coord_flip()+
xlab("")+
theme_light()
#en este caso tenemos un grafico muy aburrido porque todas tienen la misma frecuencia porque el tamaño es muy pequeño
SnowballC::wordStem(language = "spanish",words = gato$word[1:10]) #no es muy util, toma muy literal lo de la raiz
hunspell_stem(words = gato$word[1:10], dict = costa)
x = hunspell_stem(words = gato$word,dict = costa) #lematizacion
vacios = which(lapply(x, length) == 0)
gato = gato[-vacios] #las palabras divididas le quito los terminos no reconocidos #en este caso un numero
x = x[-vacios]
gato$lema = lapply(x, `[[`, 1)
vacios
gato = gato[-vacios,] #las palabras divididas le quito los terminos no reconocidos #en este caso un numero
gato = ana %>%
unnest_tokens(word, texto) %>%  #en este paso separamos las palabras individualmente
anti_join(custom_stop)   #elimina las palabras que coincidan dentro del registro de stopwords
SnowballC::wordStem(language = "spanish",words = gato$word[1:10]) #no es muy util, toma muy literal lo de la raiz
hunspell_stem(words = gato$word[1:10], dict = costa)
x = hunspell_stem(words = gato$word,dict = costa) #lematizacion
vacios = which(lapply(x, length) == 0)
gato = gato[-vacios] #las palabras divididas le quito los terminos no reconocidos #en este caso un numero
gato
gato
gato = gato[-vacios,] #las palabras divididas le quito los terminos no reconocidos #en este caso un numero
x = x[-vacios,]
x = x[-vacios]
gato$lema = lapply(x, `[[`, 1)
View(gato)
frec_simple_l=gato %>% #frecuencias de las palabras sin lematizar
count(lema,sort = T)
frec_res_l = frec_simple_l[1:15,]
frec_res_l
View(frec_res_l)
ggplot(data = frec_res,aes(x= reorder(lema,n) ,y =n))+
geom_point(color="blue")+
geom_segment(
aes(x=word,xend=word,y=0,yend=n),color="skyblue3")+
theme(axis.text.y = element_text(angle = 30,size=8,hjust=1))+
coord_flip()+
xlab("")+
theme_light()
ggplot(data = frec_res_l,aes(x= reorder(lema,n) ,y =n))+
geom_point(color="blue")+
geom_segment(
aes(x=word,xend=word,y=0,yend=n),color="skyblue3")+
theme(axis.text.y = element_text(angle = 30,size=8,hjust=1))+
coord_flip()+
xlab("")+
theme_light()
ggplot(data = frec_res_l,aes(x= reorder(lema,n) ,y =n))+
geom_point(color="blue")
reorder(lema,n)
View(frec_res_l)
ggplot(data = frec_res_l,aes(x= reorder(lema,n) ,y =n))+
geom_point(color="blue")
frec_res_l
View(frec_res_l)
gato$lema = lapply(x, `[[`, 1) %>% as.character()
frec_simple_l=gato %>% #frecuencias de las palabras sin lematizar
count(lema,sort = T)
frec_res_l = frec_simple_l[1:15,]
ggplot(data = frec_res_l,aes(x= reorder(lema,n) ,y =n))+
geom_point(color="blue")
ggplot(data = frec_res_l,aes(x= reorder(lema,n) ,y =n))+
geom_point(color="blue")+
geom_segment(
aes(x=word,xend=word,y=0,yend=n),color="skyblue3")+
theme(axis.text.y = element_text(angle = 30,size=8,hjust=1))+
coord_flip()+
xlab("")+
theme_light()
ggplot(data = frec_res_l,aes(x= reorder(lema,n) ,y =n))+
geom_point(color="blue")+
geom_segment(
aes(x=word,xend=word,y=0,yend=n),color="skyblue3")#+
ggplot(data = frec_res_l,aes(x= reorder(lema,n) ,y =n))+
geom_point(color="blue")+
geom_segment(
aes(x=lema,xend=lema,y=0,yend=n),color="skyblue3")+ #cuidado, veamos que x y xend ahora son lema
theme(axis.text.y = element_text(angle = 30,size=8,hjust=1))+
coord_flip()+
xlab("")+
theme_light()
