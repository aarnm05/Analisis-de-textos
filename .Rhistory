dataset <- read_excel("Sortone/OneDrive/U/Encuestas/PreguntaAbierta/dataset.xlsx")
library(dplyr)
library(dplyr)
library(tidytext)
library(hunspell)
library(readxl)
hunspell::list_dictionaries()
dictionary(lang = "es_cr")
dict.esp <- dictionary("es_cr")
dataset <- read_excel("Sortone/OneDrive/U/Encuestas/PreguntaAbierta/dataset.xlsx")
setwd("~/Sortone/OneDrive/Git/Analisis-de-textos")
dataset <- read_excel("Sortone/OneDrive/U/Encuestas/PreguntaAbierta/dataset.xlsx")
library(readxl)
dataset <- read_excel("~/dataset.xlsx")
dataset <- read_excel("/dataset.xlsx")
dataset <- read_excel("dataset.xlsx")
hunspell(dataset$P1COM,dict = "es_cr")
k = hunspell(dataset$P1COM,dict = "es_cr")
View(k)
k[[1]]
View(k)
k[[48]]
k = hunspell(dataset$P1COM,dict = dict.esp)
View(k)
View(k)
dataset$P1COM[1]
hunspell(dataset$P1COM[1],dict = dict.esp)
k = hunspell(dataset$P1COM[1],dict = dict.esp)
View(k)
k[[1]]
View(dataset)
hunspell(dataset$P1COM[1],dict = dict.esp)
hunspell("kars")
hunspell::list_dictionaries()
hunspell("kars r gud",dict = "en_US")
hunspell("kars are gud",dict = "en_US")
hunspell("kars are very gud",dict = "en_US")
dataset$P1COM[1]
hunspell(dataset$P1COM[1],dict = dict.esp)
hunspell::list_dictionaries()
hunspell::list_dictionaries()
dict.esp <- dictionary("es_ES")
library(dplyr)
library(dplyr)
library(tidytext)
library(dplyr)
library(tidytext)
library(hunspell)
library(readxl)
hunspell::list_dictionaries()
dict.esp <- dictionary("es_ES")
dict.esp <- dictionary("es_ES")
dataset <- read_excel("dataset.xlsx")
k = hunspell(dataset$P1COM[1],dict = dict.esp)
hunspell(dataset$P1COM[1],dict = dict.esp)
hunspell(dataset$P1COM[1],dict = dict.esp)
library(dplyr)
library(dplyr)
library(tidytext)
library(dplyr)
library(tidytext)
library(hunspell)
library(readxl)
hunspell::list_dictionaries()
hunspell(dataset$P1COM[1],dict = dict.esp)
hunspell::list_dictionaries()
espa=dictionary("~/espanol")
espa=dictionary("~/espanol/")
espa=dictionary("~/espanol/es_ES.dic")
setwd("~/Sortone/OneDrive/Git/Analisis-de-textos")
espa=dictionary("~/espanol/es_ES.dic")
espa=dictionary("C:/Users/emirr/Documents/Sortone/OneDrive/Git/Analisis-de-textos/espanol/es_ES.dic")
espa=dictionary("C:/Users/emirr/Documents/Sortone/OneDrive/Git/Analisis-de-textos/espanol/es_ES.dic")
hunspell::list_dictionaries()
espa
espa
espa=dictionary("C:/Users/emirr/Documents/Sortone/OneDrive/Git/Analisis-de-textos/espanol/es_ES.aff")
espa
espa=dictionary("C:/Users/emirr/Documents/Sortone/OneDrive/Git/Analisis-de-textos/espanol/es_ES.dic")
espa
espa=dictionary("C:/Users/emirr/Documents/Sortone/OneDrive/Git/Analisis-de-textos/mex/")
espa=dictionary("C:/Users/emirr/Documents/Sortone/OneDrive/Git/Analisis-de-textos/mex/es_MX.dic")
espa
hunspell(dataset$P1COM[1],dict = espa)
costa = dictionary("C:/Users/emirr/Documents/Sortone/OneDrive/Git/Analisis-de-textos/costa/es_cr.dic")
hunspell(dataset$P1COM[1],dict = costa)
hunspell(dataset$P1COM,dict = costa)
k = hunspell(dataset$P1COM,dict = costa)
View(k)
b = hunspell_suggest(k,dict=costa)
View(k)
k[[]]
k[]
b = hunspell_suggest(k[],dict=costa)
k[[1]]
hunspell_suggest(k[[1]],dict=costa)
hunspell_suggest(k[[1]],dict= espa)
cast = dictionary("C:/Users/emirr/Documents/Sortone/OneDrive/Git/Analisis-de-textos/costa/es_ES.dic")
cast = dictionary("C:/Users/emirr/Documents/Sortone/OneDrive/Git/Analisis-de-textos/cast/es_ES.dic")
hunspell_suggest(k[[1]],dict= cast)
hunspell_suggest(k[[1]],dict= costa)
k
k = hunspell(dataset$P1COM,dict = costa)
hunspell_suggest(k[[1]],dict= costa)
hunspell_suggest(k[[1]],dict= costa)[[1]][1]
hunspell_suggest(k[[1]],dict= costa)
k
k
View(k)
k[[1]]
hunspell_suggest(k[[1]],dict= costa)
hunspell_suggest(k[[1]],dict= costa)[[2]]
k[[1]]
k[[1]]
k
k[[1]] %>%
as.data.frame()
r = k[[1]] %>%
as.data.frame()
View(r)
r = k %>%
as.data.frame()
r = k %>%
as.data.frame()
View(r)
View(k)
cleantext = function(x) {
sapply(1:length(x), function(y) {
bad = hunspell(x[y], dict = costa)[[1]]
good = unlist(lapply(hunspell_suggest(bad), `[[`, 1))
if (length(bad)) {
for (i in 1:length(bad)) {
x[y] <<- gsub(bad[i], good[i], x[y])
}
}
})
x
}
cleantext(dataset$P1COM)
dataset$P1COM[1]
cleantext(dataset$P1COM[1])
cleantext = function(x) {
sapply(1:length(x), function(y) {
bad = hunspell(x[y], dict = costa)[[1]]
good = unlist(lapply(hunspell_suggest(bad,dict = costa), `[[`, 1))
if (length(bad)) {
for (i in 1:length(bad)) {
x[y] <<- gsub(bad[i], good[i], x[y])
}
}
})
x
}
cleantext(dataset$P1COM[1])
cleantext(dataset$P1COM[2])
library(dplyr)
library(tidytext)
library(hunspell)
library(readxl)
library(tidytext)
library(SnowballC)
library(stringr)
hunspell::list_dictionaries()
#modifiquen esto a conveniencia, en mi git estan esos diccionarios
espa=dictionary("C:/Users/emirr/Documents/Sortone/OneDrive/Git/Analisis-de-textos/mex/es_MX.dic")
costa = dictionary("C:/Users/emirr/Documents/Sortone/OneDrive/Git/Analisis-de-textos/costa/es_cr.dic")
cast = dictionary("C:/Users/emirr/Documents/Sortone/OneDrive/Git/Analisis-de-textos/cast/es_ES.dic")
dataset <- foreign::read.spss("PILOTO.sav",to.data.frame = T) #mi dataset
otro  = str_trim(dataset$PR10B) #le quitamos los espacios de mas
otro = otro[otro!=""] #quitamos las variables vacias basicamente
otro #podemos ver que hay problemas por el !.
hunspell(otro,dict = costa) #prueba de como funciona hunspell
#adaptado del siguiente codigo
#https://stackoverflow.com/questions/56026550/how-to-use-hunspell-package-to-suggest-correct-words-in-a-column-in-r
cleantext = function(x) {
sapply(1:length(x), function(y) {
bad = hunspell(x[y], dict = costa)[[1]]
good = unlist(lapply(hunspell_suggest(bad,dict = costa), `[[`, 1))
if (length(bad)) {
for (i in 1:length(bad)) {
x[y] <<- gsub(bad[i], good[i], x[y])
}
}
})
x
}
#plan de contingencia
#ejemplo
ar= str_replace("nataci!n","\\!",replacement = "e") #basicamente vamos a hacer esto, reemplazar el signo de exclamacion por una vocal
ar
cleantext(ar)
remove(ar)
#fin del ejemplo
#hacemos esto porque la funcion toma un signo de exclamacion como el fin de una oracion (de forma correcta segun el español); consecuentemente lo va a tomar como si fuera dos palabras.
otro1 = str_replace_all(otro,"\\!", replacement = "e") #puede que no les funcione dependiendo de su version de R. En ese caso tendrían que borrarle un backslash.
otro1 = cleantext(otro1)
#fin de plan de contingencia
ana = data.frame(texto = otro1, id = 1:length(otro1))
custom_stop= data_frame(word = tm::stopwords("spanish"), #podemos hacerlo así
lexicon = "custom")
gato = ana %>%
unnest_tokens(word, texto) %>%  #en este paso separamos las palabras individualmente
anti_join(custom_stop)   #elimina las palabras que coincidan dentro del registro de stopwords
#gato tiene las palabras sin lematizar separadas, gato sirve para casos donde no queremos lematizar
frec_persona = gato %>% #frecuencias de las palabras sin lematizar
group_by(id) %>% #si quisieramos por persona para controlar si alguien dijo muchas veces una palabra
count(word,sort = T)
frec_simple=gato %>% #frecuencias de las palabras sin lematizar
count(word,sort = T)
frec_res = frec_simple[1:15,]
library(ggplot2)
ggplot(data = frec_res,aes(x= reorder(word,n) ,y =n))+
geom_point(color="blue")+
geom_segment(
aes(x=word,xend=word,y=0,yend=n),color="skyblue3")+
theme(axis.text.y = element_text(angle = 30,size=8,hjust=1))+
coord_flip()+
xlab("")+
theme_light()
#en este caso tenemos un grafico muy aburrido porque todas tienen la misma frecuencia porque el tamaño es muy pequeño
gato
data = data_frame(txt = sample(terms, 100, replace = TRUE))
terms <- c("emails are nice", "emailing is fun", "computer freaks", "broken modem")
data = data_frame(txt = sample(terms, 100, replace = TRUE))
data %>%
unnest_tokens(word, txt) %>%
mutate(word = wordStem(word))
terms <- c("emails are nice", "emailing is fun", "computer freaks", "broken modem")
data = data_frame(txt = terms)
data
data %>%
unnest_tokens(word, txt) %>%
mutate(word = wordStem(word))
ara = data %>%
unnest_tokens(word, txt) %>%
mutate(word = wordStem(word))
View(ara)
getStemLanguages()
terms <- c("salgo a correr","comemos juntos","corremos","salimos")
data = data_frame(txt = terms)
ara = data %>%
unnest_tokens(word, txt) %>%
mutate(word = wordStem(word,language = "spanish"))
View(ara)
hunspell::list_dictionaries()
#modifiquen esto a conveniencia, en mi git estan esos diccionarios
espa=dictionary("C:/Users/emirr/Documents/Sortone/OneDrive/Git/Analisis-de-textos/mex/es_MX.dic")
costa = dictionary("C:/Users/emirr/Documents/Sortone/OneDrive/Git/Analisis-de-textos/costa/es_cr.dic")
cast = dictionary("C:/Users/emirr/Documents/Sortone/OneDrive/Git/Analisis-de-textos/cast/es_ES.dic")
hunspell::list_dictionaries()
hunspell::list_dictionaries()
hunspell::dictionary("C:/Users/emirr/Documents/Sortone/OneDrive/Git/Analisis-de-textos/costa/es_cr.dic")
hunspell::list_dictionaries()
hunspell::dicpath("C:/Users/emirr/Documents/Sortone/OneDrive/Git/Analisis-de-textos/costa/es_cr.dic")
costa
espa
hunspell_parse(data,format = "latex")
terms <- c("salgo a correr","comemos juntos","corremos","salimos")
hunspell_parse(terms,format = "latex")
hunspell_stem(words = terms, dict = costa)
terms <- c("salgo a correr","comemos juntos","corremos","salimos")
hunspell_stem(words = terms, dict = costa)
hunspell_stem(words = gato$word, dict = costa)
gato
hunspell_stem(words = gato$word[1:5], dict = costa)
gato$word[1:5]
hunspell_stem(words = gato$word[1:5], dict = costa)
gato$word[1:5]
hunspell_stem(words = gato$word[1:10], dict = costa)
hunspell_stem(words = gato$word[1:10], dict = costa)
gato$word[1:10]
ara = hunspell_stem(words = gato$word[1:10], dict = costa) %>%
unlist()
ara = hunspell_stem(words = gato$word[1:10], dict = costa)
View(ara)
ara = hunspell_stem(words = gato$word[1:10], dict = costa) %>%
data.frame()
View(ana)
View(ara)
ara = hunspell_stem(words = gato$word[1:10], dict = costa) %>%
unlist()
ara
ara = hunspell_stem(words = gato$word[1:10], dict = costa)
View(ara)
select(gato,"word") %>%  #necesita un poco de trabajo todavia
SnowballC::wordStem(language = "spanish")
SnowballC::wordStem(language = "spanish",words = gato$word[1:10])
hunspell_stem(words = gato$word[1:10], dict = costa)
hunspell_stem(words = gato$word[6],dict = costa)
hunspell_stem(words = gato$word[6],dict = costa)[[1]]
hunspell_stem(words = gato$word[6],dict = costa)[[1]][1]
lema = gato %>%
mutate(lemas =hunspell_stem(words = word,dict = costa)[[1]][1] )
View(lema)
lema = gato %>%
mutate(lemas =hunspell_stem(words = word,dict = costa)[[]][1] )
lema = gato %>%
mutate(lemas =hunspell_stem(words = word,dict = costa) )
View(lema)
hunspell_stem(words = gato$word,dict = costa)[1]
lema = gato %>%
mutate(lemas =hunspell_stem(words = word,dict = costa)[1] )
lemas =hunspell_stem(words = word,dict = costa)[[
hunspell_stem(words = gato$word[1:4],dict = costa)[[
hunspell_stem(words = gato$word[1:4],dict = costa)[[ 1
lapply ( hunspell_stem(words = gato$word[1:4],dict = costa), "[[",1)
x= hunspell_stem(words = gato$word[1:4],dict = costa)
lapply(x, `[[`, 1)
A = lapply(x, `[[`, 1)
View(A)
A = lapply(x, `[[`, 1) %>%
data.frame()
View(A)
A = lapply(x, `[[`, 1) %>%
data.frame() %>% t()
View(A)
