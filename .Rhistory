Our_bigram2 = lema_bigrama %>%
unnest_tokens(bigram, respuesta, token = "ngrams", n = 2)
Our_bigram2 = lema_bigrama %>%
unnest_tokens(bigram, lema, token = "ngrams", n = 2)
View(Our_bigram)
View(Our_bigram2)
View(texto.df)
Our_bigram2 = lema_bigrama %>%
unnest_tokens(bigram, lema, token = "ngrams", n = 2)
#crear bigramas
Our_bigram = texto.df %>%
unnest_tokens(bigram, respuesta, token = "ngrams", n = 2)
View(Our_bigram)
View(lema_bigrama)
Our_bigram2 = lema_bigrama %>%
select(lema) %>%
unnest_tokens(bigram, lema, token = "ngrams", n = 2)
#grafico de la tesis, mas bonito
word_cors <- texto.df %>%
mutate(section = row_number()) %>%
unnest_tokens(word, respuesta,
token = "ngrams",
n = 1)  %>%
group_by(word) %>%
filter(n() >= 5) %>%
widyr::pairwise_cor(word, section,
sort = TRUE)
word_cors
word_cors2 <- lema_bigrama %>%
mutate(section = row_number()) %>%
unnest_tokens(word, respuesta,
token = "ngrams",
n = 1)  %>%
group_by(word) %>%
filter(n() >= 5) %>%
widyr::pairwise_cor(word, section,
sort = TRUE)
word_cors2 <- lema_bigrama %>%
mutate(section = row_number()) %>%
unnest_tokens(word, lema, token = "ngrams", n = 1)  %>%
group_by(word) %>%
filter(n() >= 5) %>%
widyr::pairwise_cor(word, section,
sort = TRUE)
word_cors2
word_cors2
word_cors
ggraph(bigrama_graf, layout = "fr") +
geom_edge_link() +
geom_node_point() +
geom_node_text(aes(label = name), vjust = 1, hjust = 1)
word_cors %>%
filter(correlation >= .25) %>%
graph_from_data_frame() %>%
ggraph(layout = "fr") +
geom_edge_link(aes(#edge_alpha = correlation,
edge_width = correlation,
colour = correlation)) +
geom_node_point(color = "#29e05d7a", size = 5) +
geom_node_text(aes(label = name), repel = TRUE) +
theme_void()
ggraph(bigrama_graf, layout = "fr") +
geom_edge_link() +
geom_node_point() +
geom_node_text(aes(label = name), vjust = 1, hjust = 1)
word_cors %>%
filter(correlation >= .25) %>%
graph_from_data_frame() %>%
ggraph(layout = "fr") +
geom_edge_link(aes(#edge_alpha = correlation,
edge_width = correlation,
colour = correlation)) +
geom_node_point(color = "#29e05d7a", size = 5) +
geom_node_text(aes(label = name), repel = TRUE) +
theme_void()
word_cors %>%
filter( abs(correlation ) >= .25) %>%
graph_from_data_frame() %>%
ggraph(layout = "fr") +
geom_edge_link(aes(#edge_alpha = correlation,
edge_width = correlation,
colour = correlation)) +
geom_node_point(color = "#29e05d7a", size = 5) +
geom_node_text(aes(label = name), repel = TRUE) +
theme_void()
ggraph(bigrama_graf, layout = "fr") +
geom_edge_link() +
geom_node_point() +
geom_node_text(aes(label = name), vjust = 1, hjust = 1)
word_cors %>%
filter(correlation >= .25) %>%
graph_from_data_frame() %>%
ggraph(layout = "fr") +
geom_edge_link(aes(#edge_alpha = correlation,
edge_width = correlation,
colour = correlation)) +
geom_node_point(color = "#29e05d7a", size = 5) +
geom_node_text(aes(label = name), repel = TRUE) +
theme_void()
word_cors %>%
filter( abs(correlation ) >= .25) %>%
graph_from_data_frame() %>%
ggraph(layout = "fr") +
geom_edge_link(aes(#edge_alpha = correlation,
edge_width = correlation,
colour = correlation)) +
geom_node_point(color = "#29e05d7a", size = 5) +
geom_node_text(aes(label = name), repel = TRUE) +
theme_void()
Our_bigram_count[1:15,] %>%
ggplot(aes(x = reorder(bigram,n) ,y = n))+
geom_point(color = "blue")+
geom_segment(
aes(x=bigram,xend=bigram,y=0,yend=n),color="skyblue3")+
theme(axis.text.y = element_text(angle = 30,size=8,hjust=1))+
coord_flip()+
xlab("")+
theme_light()+
labs(title = "Temas principales en las respuestas de los participantes",
subtitle = "Grafico de frecuencia por palabra, con bigramas",
y = "Cantidad")+
theme(plot.title = element_text( hjust = 0.5),
plot.subtitle = element_text( hjust = 0.5),
panel.grid.major.y = element_blank(),
panel.border = element_blank(),
axis.ticks.y = element_blank())
ggraph(bigrama_graf, layout = "fr") +
geom_edge_link() +
geom_node_point() +
geom_node_text(aes(label = name), vjust = 1, hjust = 1)
word_cors %>%
filter(correlation >= .25) %>%
graph_from_data_frame() %>%
ggraph(layout = "fr") +
geom_edge_link(aes(#edge_alpha = correlation,
edge_width = correlation,
colour = correlation)) +
geom_node_point(color = "#29e05d7a", size = 5) +
geom_node_text(aes(label = name), repel = TRUE) +
theme_void()
word_cors %>%
filter( abs(correlation ) >= .25) %>%
graph_from_data_frame() %>%
ggraph(layout = "fr") +
geom_edge_link(aes(#edge_alpha = correlation,
edge_width = correlation,
colour = correlation)) +
geom_node_point(color = "#29e05d7a", size = 5) +
geom_node_text(aes(label = name), repel = TRUE) +
theme_void()
Our_bigram_count[1:15,] %>%
ggplot(aes(x = reorder(bigram,n) ,y = n))+
geom_point(color = "blue")+
geom_segment(
aes(x=bigram,xend=bigram,y=0,yend=n),color="skyblue3")+
theme(axis.text.y = element_text(angle = 30,size=8,hjust=1))+
coord_flip()+
xlab("")+
theme_light()+
labs(title = "Temas principales en las respuestas de los participantes",
subtitle = "Grafico de frecuencia por palabra, con bigramas",
y = "Cantidad")+
theme(plot.title = element_text( hjust = 0.5),
plot.subtitle = element_text( hjust = 0.5),
panel.grid.major.y = element_blank(),
panel.border = element_blank(),
axis.ticks.y = element_blank())
Our_bigram_count %>%
Our_bigram_count
graf_bigrama = Our_bigram_count[1:15,]
View(graf_bigrama)
colnames(graf_bigrama) = c("word","n")
grafico_palabras(a = graf_bigrama,
titulo = "Temas principales en las respuestas de los participantes",
subtitulo = "Grafico de frecuencia por palabra, con bigramas")
Our_bigram_count[1:15,] %>%
ggplot(aes(x = reorder(bigram,n) ,y = n))+
geom_point(color = "blue")+
geom_segment(
aes(x=bigram,xend=bigram,y=0,yend=n),color="skyblue3")+
theme(axis.text.y = element_text(angle = 30,size=8,hjust=1))+
coord_flip()+
xlab("")+
theme_light()+
labs(title = "Temas principales en las respuestas de los participantes",
subtitle = "Grafico de frecuencia por palabra, con bigramas",
y = "Cantidad")+
theme(plot.title = element_text( hjust = 0.5),
plot.subtitle = element_text( hjust = 0.5),
panel.grid.major.y = element_blank(),
panel.border = element_blank(),
axis.ticks.y = element_blank())
graf_bigrama = Our_bigram_count[1:15,]
colnames(graf_bigrama) = c("word","n")
grafico_palabras(a = graf_bigrama,
titulo = "Temas principales en las respuestas de los participantes",
subtitulo = "Grafico de frecuencia por palabra, con bigramas")
library(dplyr)
library(tidytext)
library(hunspell)
library(tidytext)
library(stringr)
library(ggplot2)
library( tidyr)
library(ggraph)
library(igraph)
#setwd("~/GitHub/Analisis-de-textos")
hunspell::dictionary("costa/es_cr.dic")
espa=dictionary("mex/es_MX.dic")
costa = dictionary("costa/es_cr.dic")
cast = dictionary("cast/es_ES.dic")
custom_stop= data_frame(word = tm::stopwords("spanish"), #stopwords
lexicon = "custom") #el nombre que le vamos a poner
custom_stop = custom_stop[custom_stop$word!="no",]
dataset <-  suppressWarnings( foreign::read.spss("Actualidades.sav",to.data.frame = T) )
#adaptado del siguiente codigo
#https://stackoverflow.com/questions/56026550/how-to-use-hunspell-package-to-suggest-correct-words-in-a-column-in-r
cleantext = function(x) {
sapply(1:length(x), function(y) {
bad = hunspell(x[y], dict = costa)[[1]]
good = unlist(lapply(hunspell_suggest(bad,dict = costa), `[[`, 1))
if (length(bad)) {
for (i in 1:length(bad)) {
x[y] <<- gsub(bad[i], good[i], x[y])
}
}
})
x
}
procesador_texto= function(x) {
otro  = str_trim(x) #le quitamos los espacios de mas
otro = otro[otro != ""] #quitamos las variables vacias basicamente
otro1 = cleantext(otro)
dummy = data.frame(texto = otro1, id = 1:length(otro1))
# Stopwords--------------------
beta = dummy %>%
unnest_tokens(word, texto) %>%  #en este paso separamos las palabras individualmente
anti_join(custom_stop)   #elimina las palabras que coincidan dentro del registro de stopwords
z = hunspell_stem(words = beta$word,dict = costa) #lematizacion
vacios = which(lapply(z, length) == 0)
beta = beta[-vacios,] #las palabras divididas le quito los terminos no reconocidos #en este caso un numero
z = z[-vacios]
beta$lema = lapply(z, `[[`, 1) %>% as.character()
beta
}
grafico_palabras = function(a,titulo,subtitulo){
ggplot(data = a,aes(x = reorder(word,n) ,y = n))+
geom_point(color = "blue")+
geom_segment(
aes(x=word,xend=word,y=0,yend=n),color="skyblue3")+
theme(axis.text.y = element_text(angle = 30,size=8,hjust=1))+
coord_flip()+
xlab("")+
theme_light()+
labs(title = titulo,
subtitle = subtitulo,
y = "Cantidad")+
theme(plot.title = element_text( hjust = 0.5),
plot.subtitle = element_text( hjust = 0.5),
panel.grid.major.y = element_blank(),
panel.border = element_blank(),
axis.ticks.y = element_blank())
}
texto_procesado = procesador_texto(dataset$PR10B)
frec_lema = texto_procesado %>%
count(lema,sort = T )
colnames(frec_lema) = c("word","n")
frec_lema15 = frec_lema[1:15,]
grafico_palabras(frec_lema15,"Temas principales en las respuestas de los participantes","Grafico de frecuencia por palabra, lematizado")
lema_bigrama = aggregate(lema~id, data = texto_procesado,paste, collapse=" ") #oraciones sin stopwords y con gramatica corregida
Our_bigram = lema_bigrama %>%
select(lema) %>%
unnest_tokens(bigram, lema, token = "ngrams", n = 2)
#contar los bigramas
Our_bigram_count = Our_bigram %>%
count(bigram, sort = TRUE)
#Separar bigramas para filtrarlos
bigramas_separados = Our_bigram_count %>%
separate( bigram, c("word1", "word2"), sep = " ")
#bigramas con la palabra "no"
bigramas_con_no = bigramas_separados %>%
filter(word1 == "no") %>%
count(word1, word2, sort = TRUE)
word_cors2 <- lema_bigrama %>%
mutate(section = row_number()) %>%
unnest_tokens(word, lema, token = "ngrams", n = 1)  %>%
group_by(word) %>%
filter(n() >= 5) %>%
widyr::pairwise_cor(word, section,
sort = TRUE)
palabras_bi = corrector.ortografia( palabras, costa)
library(dplyr)
library(tidytext)
library(hunspell)
library(tidytext)
library(stringr)
library(ggplot2)
suppressMessages(library(randomForest))
library(tidyr)
library(ggraph)
library(igraph)
library(flextable)
entrenamiento = readxl::read_xlsx("codificacion.xlsx")
dataset <-  suppressWarnings( foreign::read.spss("Actualidades_v5.sav",to.data.frame = T) )
entrenamiento$Respuesta = str_replace_all(entrenamiento$Respuesta, "covid","corona virus")
dataset$CN2 = str_replace_all(dataset$CN2,"covid","corona virus")
library(dplyr)
library(tidytext)
library(hunspell)
library(tidytext)
library(stringr)
library(ggplot2)
library( tidyr)
library(ggraph)
library(igraph)
library(flextable)
#setwd("~/GitHub/Analisis-de-textos")
hunspell::dictionary("costa/es_cr.dic")
espa=dictionary("mex/es_MX.dic")
costa = dictionary("costa/es_cr.dic")
cast = dictionary("cast/es_ES.dic")
custom_stop= data_frame(word = tm::stopwords("spanish"), #stopwords
lexicon = "custom") #el nombre que le vamos a poner
#custom_stop = custom_stop[custom_stop$word!="no",]
#adaptado del siguiente codigo
#https://stackoverflow.com/questions/56026550/how-to-use-hunspell-package-to-suggest-correct-words-in-a-column-in-r
formato_flex <- function(tabla){
tabla %>%
#autofit(add_h = 0.4) %>%#me ajusta el alto de todo, tambien haria el width, pero lo modificamos automaticamente
theme_booktabs() %>%
font(fontname= "Myriam Pro", part = "all") %>%
fontsize(size= 12, part = "all") %>%
fontsize(size = 13,i=1, part = "header") %>%
bold(bold= TRUE, part="header") %>%
align(align = "center", part = "all") %>%
align(align = "left",j=1, part = "body") %>%
align(align="justify",i=1, part = "header" ) %>%
width(j= 1, width = 3) %>%
width(j=-1, width = 1) %>%
fix_border_issues
}
cleantext = function(x) {
for (i in 1:length(x)) {
bad = hunspell(x[i], dict = costa)[[1]]
good = unlist(hunspell_suggest(bad, dict = costa))[1]
if (length(bad) > 0) {
x[i] = gsub(pattern = bad, replacement = good, x[i])
}
}
x
}
procesador_texto= function(x) {
dummy = data.frame(texto = x, id = 1:length(x))
# Stopwords--------------------
beta = dummy %>%
unnest_tokens(word, texto) %>%  #en este paso separamos las palabras individualmente
anti_join(custom_stop)
z = hunspell_stem(words = beta$word,dict = costa)
vacios = which(lapply(z, length) == 0)
beta = beta[-vacios,] #las palabras divididas le quito los terminos no reconocidos #en este caso un numero
z = z[-vacios]
beta$lema = lapply(z, `[[`, 1) %>%
as.character()
beta
}
datos_ent = cleantext(entrenamiento$Respuesta)
datos_ent2 = procesador_texto(datos_ent)
lema_ent = datos_ent2 %>%
group_by(id) %>%
count(lema)
entrenamiento1 = entrenamiento[entrenamiento$Respuesta!=99,]
matriz = cast_dtm(data = lema_ent,document = id,term = lema,value = n) %>% #los de entrenamiento
as.matrix() %>%
as.data.frame()
matriz$respuesta = entrenamiento1$Cod
matriz %>%
group_by(respuesta) %>%
summarise(Frecuencia = n()) %>%
arrange(desc(Frecuencia))
#codigo siguiente adapato de Evora
chi.cuadrado <- function(datos, umbral, respuesta = "var_respuesta")
{
num.col <- NULL
for (k in 1:dim(datos)[2])
{
#datos[ , k] <- as.numeric(unlist(datos[ , k]))
chi <- chisq.test(table(datos[ , respuesta], datos[ , k]),
simulate.p.value = TRUE)
if(chi$p.value >= umbral)
{
num.col[k] <- k
}
}
datos[ , as.vector(na.omit(num.col))] <- NULL
print(paste("El total de palabras que quedan como predictoras después de eliminar las que tiene un p-value mayor a", umbral, "son",
dim(datos)[2]-1), sep = ",")
return(datos)
}
tdm.chi <- chi.cuadrado(matriz, 0.8, "respuesta")
set.seed(12345678)
muestra <- sample(1:nrow(matriz), size = floor(nrow(matriz)*0.80)) #el 80 es arbitrario
entrenamiento.todo <- matriz[muestra , ]
test.todo <- matriz[-muestra , ]
entrenamiento.chi <- tdm.chi[muestra , ]
test.chi <- tdm.chi[-muestra , ]
entrenamiento.todo$respuesta <- as.factor(entrenamiento.todo$respuesta)
set.seed(1234)
modRF = randomForest(respuesta ~ .,
data = entrenamiento.todo,
ntree = 50,
replace = TRUE,
type = "classification")
predictionRF <- predict(modRF, test.todo)
niveles <- levels(as.factor(matriz$respuesta))
MC <- table(factor(test.todo$respuesta, levels = niveles, ordered = TRUE), #matriz de confusion
factor(predictionRF, levels = niveles, ordered = TRUE))
precision.todo <- sum(diag(MC)) / sum(MC)
paste("La precisión utilizando",
dim(entrenamiento.todo)[2], "variables es de",
scales::percent(precision.todo, accuracy = 0.1))
entrenamiento.chi$respuesta <- as.factor(entrenamiento.chi$respuesta)
set.seed(1234)
modRF = randomForest(respuesta ~ .,
data = entrenamiento.chi,
ntree = 50,
replace = TRUE,
type = "classification")
predictionRF <- predict(modRF, test.chi)
MC <- table(factor(test.chi$respuesta, levels = niveles, ordered = TRUE),
factor(predictionRF, levels = niveles, ordered = TRUE))
precision.chi <- sum(diag(MC)) / sum(MC)
paste("La precisión utilizando",
dim(entrenamiento.chi)[2], "variables es de",
scales::percent(precision.chi, accuracy = 0.1))
prop.table(table(predictionRF))*100
alt_datos = dataset %>%
select(CONSECUTIVO,CN2) %>%
mutate(CN2 = str_trim(dataset$CN2))
alt_datos = alt_datos[alt_datos$CN2!="",]
alt_datos = alt_datos[alt_datos$CN2!=9,]
alt_datos = alt_datos[alt_datos$CN2!=99,]
alt_datos$CN2 = cleantext(alt_datos$CN2)
beta = alt_datos %>%
unnest_tokens(word, CN2) %>%  #en este paso separamos las palabras individualmente
anti_join(custom_stop)
z = hunspell_stem(words = beta$word,dict = costa)
vacios = which(lapply(z, length) == 0)
beta = beta[-vacios,]
z = z[-vacios]
beta$word = lapply(z, `[[`, 1) %>% #le caemos encima a las simples con las lematizadas
as.character()
todo_texto_alt = beta %>%
group_by(CONSECUTIVO) %>%
count(word)
matriz_todo_texto_ALT = cast_dtm(data = todo_texto_alt,document = CONSECUTIVO,term = word,value = n) %>%
as.matrix() %>%
as.data.frame()
predictionRF_todo_texto <- predict(modRF, matriz_todo_texto_ALT) #va en orden segun actualidades
cat("Absoluto \n")
absolutos = table(predictionRF_todo_texto) %>% as.data.frame()
colnames(absolutos) = c("Categoría","Frecuencia")
absolutos$Error =  round(modRF$confusion[,10]*100,1)
absolutos %>%
flextable() %>%
add_header_lines(values= c("Frecuencia absoluta de las categorías, pregunta CN2")) %>%
formato_flex()
cat("\n Relativo \n")
relativo = round(prop.table(table(predictionRF_todo_texto))*100,1) %>% as.data.frame()
colnames(relativo) = c("Categoria","Porcentaje")
relativo$Error = round(modRF$confusion[,10]*100,1)
relativo %>%
flextable() %>%
add_header_lines(values= c("Frecuencia relativa de las categorías, pregunta CN2")) %>%
formato_flex()
View(modRF)
modRF[["predicted"]]
modRF[["err.rate"]]
modRF[["confusion"]]
modRF[["votes"]]
modRF[["classes"]]
modRF[["importance"]]
modRF[["forest"]][["treemap"]]
modRF[["y"]]
modRF[["terms"]]
modRF[["terms"]][[2]]
modRF[["terms"]][[2]]
MC
predictionRF
MC
test.todo$respuesta
predictionRF
getTree(modRF)
getTree(modRF,labelVar = T)
arbol = getTree(modRF,labelVar = T)
View(arbol)
arbol[is.na(arbol$prediction)=F,]
arbol[is.na(arbol$prediction)==F,]
predictores = arbol[is.na(arbol$prediction)==F,]
View(predictores)
remove(predictores)
arbol = getTree(modRF,labelVar = F)
View(arbol)
arbol = getTree(modRF,labelVar = T)
arbol
arbol = getTree(modRF,labelVar = T)
View(arbol)
View(arbol)
arbol = getTree(modRF,k = 40,labelVar = T)
arbol = getTree(modRF,k = 20,labelVar = T)
arbol = getTree(modRF,k = 50,labelVar = T)
modRF$votes
View(modRF)
