anali_pr10b = procesador_texto(dataset$PR10B) # me devuelve las 15 palabras más frecuentes sin lematizar
View(anali_pr10b)
ggplot(data = anali_pr10b,aes(x= reorder(word,n) ,y =n))+
geom_point(color="blue")+
geom_segment(
aes(x=word,xend=word,y=0,yend=n),color="skyblue3")+
theme(axis.text.y = element_text(angle = 30,size=8,hjust=1))+
coord_flip()+
xlab("")+
theme_light()+
labs(title = "Temas principales en las respuestas de los participantes",
subtitle = "Grafico de frecuencia por palabra, sin lematizar",
y = "Cantidad")+
#scale_x_discrete(breaks = NULL)+
theme(plot.title = element_text( hjust = 0.5),
plot.subtitle = element_text( hjust = 0.5),
panel.grid.major.y = element_blank(),
panel.border = element_blank(),
axis.ticks.y = element_blank())
#este grafico sirve para las palabras que no queremos lematizar. Como los nombres propios.
library(ggplot2)
ggplot(data = anali_pr10b,aes(x= reorder(word,n) ,y =n))+
geom_point(color="blue")+
geom_segment(
aes(x=word,xend=word,y=0,yend=n),color="skyblue3")+
theme(axis.text.y = element_text(angle = 30,size=8,hjust=1))+
coord_flip()+
xlab("")+
theme_light()+
labs(title = "Temas principales en las respuestas de los participantes",
subtitle = "Grafico de frecuencia por palabra, sin lematizar",
y = "Cantidad")+
#scale_x_discrete(breaks = NULL)+
theme(plot.title = element_text( hjust = 0.5),
plot.subtitle = element_text( hjust = 0.5),
panel.grid.major.y = element_blank(),
panel.border = element_blank(),
axis.ticks.y = element_blank())
grafico_palabras = function(a,titulo,subtitulo){
ggplot(data = a,aes(x= reorder(word,n) ,y =n))+
geom_point(color="blue")+
geom_segment(
aes(x=word,xend=word,y=0,yend=n),color="skyblue3")+
theme(axis.text.y = element_text(angle = 30,size=8,hjust=1))+
coord_flip()+
xlab("")+
theme_light()+
labs(title = titutlo,
subtitle = subtitulo,
y = "Cantidad")+
theme(plot.title = element_text( hjust = 0.5),
plot.subtitle = element_text( hjust = 0.5),
panel.grid.major.y = element_blank(),
panel.border = element_blank(),
axis.ticks.y = element_blank())
}
grafico_palabras(anali_pr10b,"Temas principales en las respuestas de los participantes","Grafico de frecuencia por palabra, sin lematizar")
grafico_palabras = function(a,titulo,subtitulo){
ggplot(data = a,aes(x= reorder(word,n) ,y =n))+
geom_point(color="blue")+
geom_segment(
aes(x=word,xend=word,y=0,yend=n),color="skyblue3")+
theme(axis.text.y = element_text(angle = 30,size=8,hjust=1))+
coord_flip()+
xlab("")+
theme_light()+
labs(title = titulo,
subtitle = subtitulo,
y = "Cantidad")+
theme(plot.title = element_text( hjust = 0.5),
plot.subtitle = element_text( hjust = 0.5),
panel.grid.major.y = element_blank(),
panel.border = element_blank(),
axis.ticks.y = element_blank())
}
grafico_palabras(anali_pr10b,"Temas principales en las respuestas de los participantes","Grafico de frecuencia por palabra, sin lematizar")
procesador_texto = function(x) {
otro  = str_trim(x) #le quitamos los espacios de mas
otro = otro[otro != ""] #quitamos las variables vacias basicamente
otro1 = cleantext(otro)
#View(cbind(otro,otro1)) #por si quieren comparar los resultados
ana = data.frame(texto = otro1, id = 1:length(otro1)) #si quisieramos podriamos conservar el id original del cuestionario. Para esto seleccionariamos las dos columnas y luego le haríamos un mutate o algo similar
# Stopwords--------------------
gato = ana %>%
unnest_tokens(word, texto) %>%  #en este paso separamos las palabras individualmente
anti_join(custom_stop)   #elimina las palabras que coincidan dentro del registro de stopwords
#gato tiene las palabras sin lematizar separadas, gato sirve para casos donde no queremos lematizar
frec_persona = gato %>% #frecuencias de las palabras sin lematizar
group_by(id) %>% #si quisieramos por persona para controlar si alguien dijo muchas veces una palabra
count(word, sort = T) # a final de cuentas creo que esto no va a ser necesario. la muestra es muy grande como para que influya significativamente. Si tuvieramos ~40 observaciones se podría hacer.
frec_simple = gato %>% #frecuencias de las palabras sin lematizar
count(word, sort = T)
frec_simple
}
anali_pr10b = procesador_texto(dataset$PR10B) # me devuelve las 15 palabras más frecuentes sin lematizar
anali_pr10b = procesador_texto(dataset$PR10B)[1:15,] # me devuelve las 15 palabras más frecuentes sin lematizar
anali_pr10b = procesador_texto(dataset$PR10B) # me devuelve las 15 palabras más frecuentes sin lematizar
View(anali_pr10b)
pr10b_proc = procesador_texto(dataset$PR10B) # me devuelve las palabras ordenadas por frecuencia
pr10b_proc15 = pr10b_proc[1:15,]
View(pr10b_proc)
View(pr10b_proc15)
ggplot(data = pr10b_proc15,aes(x= reorder(word,n) ,y =n))+
geom_point(color="blue")+
geom_segment(
aes(x=word,xend=word,y=0,yend=n),color="skyblue3")+
theme(axis.text.y = element_text(angle = 30,size=8,hjust=1))+
coord_flip()+
xlab("")+
theme_light()+
labs(title = "Temas principales en las respuestas de los participantes",
subtitle = "Grafico de frecuencia por palabra, sin lematizar",
y = "Cantidad")+
#scale_x_discrete(breaks = NULL)+
theme(plot.title = element_text( hjust = 0.5),
plot.subtitle = element_text( hjust = 0.5),
panel.grid.major.y = element_blank(),
panel.border = element_blank(),
axis.ticks.y = element_blank())
grafico_palabras = function(a,titulo,subtitulo){
ggplot(data = a,aes(x= reorder(word,n) ,y =n))+
geom_point(color="blue")+
geom_segment(
aes(x=word,xend=word,y=0,yend=n),color="skyblue3")+
theme(axis.text.y = element_text(angle = 30,size=8,hjust=1))+
coord_flip()+
xlab("")+
theme_light()+
labs(title = titulo,
subtitle = subtitulo,
y = "Cantidad")+
theme(plot.title = element_text( hjust = 0.5),
plot.subtitle = element_text( hjust = 0.5),
panel.grid.major.y = element_blank(),
panel.border = element_blank(),
axis.ticks.y = element_blank())
}
#asi podemos aplicarlo más fácil para varias frecuencias de palabras
grafico_palabras(pr10b_proc15,"Temas principales en las respuestas de los participantes","Grafico de frecuencia por palabra, sin lematizar")
x = hunspell_stem(words = pr10b_proc$word,dict = costa) #lematizacion
View(x)
sugerencias = hunspell_stem(words = pr10b_proc$word,dict = costa) #lematizacion
sugerencias = sugerencias[-which(lapply(x, length) == 0),] #las palabras divididas le quito los terminos no reconocidos
sugerencias = sugerencias[-which(lapply(sugerencias, length) == 0),] #las palabras divididas le quito los terminos no reconocidos
which(lapply(sugerencias, length) == 0)
View(sugerencias)
lemas = pr10b_proc[-which(lapply(sugerencias, length) == 0),] #las palabras divididas le quito los terminos no reconocidos
View(lemas)
View(sugerencias)
procesador_texto_lema = function(x) {
otro  = str_trim(x) #le quitamos los espacios de mas
otro = otro[otro != ""] #quitamos las variables vacias basicamente
otro1 = cleantext(otro)
#View(cbind(otro,otro1)) #por si quieren comparar los resultados
ana = data.frame(texto = otro1, id = 1:length(otro1)) #si quisieramos podriamos conservar el id original del cuestionario. Para esto seleccionariamos las dos columnas y luego le haríamos un mutate o algo similar
# Stopwords--------------------
gato = ana %>%
unnest_tokens(word, texto) %>%  #en este paso separamos las palabras individualmente
anti_join(custom_stop)   #elimina las palabras que coincidan dentro del registro de stopwords
#gato tiene las palabras sin lematizar separadas, gato sirve para casos donde no queremos lematizar
z = hunspell_stem(words = gato$word,dict = costa) #lematizacion
gato2 = gato[-which(lapply(z, length) == 0),] #las palabras divididas le quito los terminos no reconocidos
z = z[-vacios]
gato$lema = lapply(z, `[[`, 1) %>% as.character()
}
ala = procesador_texto_lema(dataset$PR10B)
procesador_texto_lema = function(x) {
otro  = str_trim(x) #le quitamos los espacios de mas
otro = otro[otro != ""] #quitamos las variables vacias basicamente
otro1 = cleantext(otro)
#View(cbind(otro,otro1)) #por si quieren comparar los resultados
ana = data.frame(texto = otro1, id = 1:length(otro1)) #si quisieramos podriamos conservar el id original del cuestionario. Para esto seleccionariamos las dos columnas y luego le haríamos un mutate o algo similar
# Stopwords--------------------
gato = ana %>%
unnest_tokens(word, texto) %>%  #en este paso separamos las palabras individualmente
anti_join(custom_stop)   #elimina las palabras que coincidan dentro del registro de stopwords
#gato tiene las palabras sin lematizar separadas, gato sirve para casos donde no queremos lematizar
z = hunspell_stem(words = gato$word,dict = costa) #lematizacion
vacios = which(lapply(z, length) == 0)
gato2 = gato[-vacios,] #las palabras divididas le quito los terminos no reconocidos
z = z[-vacios]
gato$lema = lapply(z, `[[`, 1) %>% as.character()
}
ala = procesador_texto_lema(dataset$PR10B)
procesador_texto_lema = function(x) {
otro  = str_trim(x) #le quitamos los espacios de mas
otro = otro[otro != ""] #quitamos las variables vacias basicamente
otro1 = cleantext(otro)
#View(cbind(otro,otro1)) #por si quieren comparar los resultados
ana = data.frame(texto = otro1, id = 1:length(otro1)) #si quisieramos podriamos conservar el id original del cuestionario. Para esto seleccionariamos las dos columnas y luego le haríamos un mutate o algo similar
# Stopwords--------------------
gato = ana %>%
unnest_tokens(word, texto) %>%  #en este paso separamos las palabras individualmente
anti_join(custom_stop)   #elimina las palabras que coincidan dentro del registro de stopwords
#gato tiene las palabras sin lematizar separadas, gato sirve para casos donde no queremos lematizar
z = hunspell_stem(words = gato$word,dict = costa) #lematizacion
vacios = which(lapply(z, length) == 0)
gato = gato[-vacios,] #las palabras divididas le quito los terminos no reconocidos #en este caso un numero
z = z[-vacios]
gato$lema = lapply(z, `[[`, 1) %>% as.character()
}
ala = procesador_texto_lema(dataset$PR10B)
procesador_texto_lema = function(x) {
otro  = str_trim(x) #le quitamos los espacios de mas
otro = otro[otro != ""] #quitamos las variables vacias basicamente
otro1 = cleantext(otro)
#View(cbind(otro,otro1)) #por si quieren comparar los resultados
ana = data.frame(texto = otro1, id = 1:length(otro1)) #si quisieramos podriamos conservar el id original del cuestionario. Para esto seleccionariamos las dos columnas y luego le haríamos un mutate o algo similar
# Stopwords--------------------
gato = ana %>%
unnest_tokens(word, texto) %>%  #en este paso separamos las palabras individualmente
anti_join(custom_stop)   #elimina las palabras que coincidan dentro del registro de stopwords
#gato tiene las palabras sin lematizar separadas, gato sirve para casos donde no queremos lematizar
z = hunspell_stem(words = gato$word,dict = costa) #lematizacion
vacios = which(lapply(z, length) == 0)
gato = gato[-vacios,] #las palabras divididas le quito los terminos no reconocidos #en este caso un numero
z = z[-vacios]
gato$lema = lapply(z, `[[`, 1) %>% as.character()
gato
}
ala = procesador_texto_lema(dataset$PR10B)
View(ala)
frec_simple = ala %>% #frecuencias de las palabras sin lematizar
count(lema, sort = T)
View(frec_simple)
View(ala)
View(ala)
frec_simple = ala %>% #frecuencias de las palabras lematizadas
count(lema, sort = T)
frec_simple_lema = ala %>%
count(word,sort = T )
library(dplyr)
library(tidytext)
library(hunspell)
library(readxl)
library(tidytext)
library(SnowballC)
library(stringr)
#setwd("~/GitHub/Analisis-de-textos")
hunspell::dictionary("costa/es_cr.dic")
#modifiquen esto a conveniencia, en mi git estan esos diccionarios. Fernando me preguntó por qué había tantos diccionarios. Es solo para probar
espa=dictionary("mex/es_MX.dic")
costa = dictionary("costa/es_cr.dic")
cast = dictionary("cast/es_ES.dic")
custom_stop= data_frame(word = tm::stopwords("spanish"), #stopwords
lexicon = "custom") #el nombre que le vamos a poner
dataset <- foreign::read.spss("Actualidades.sav",to.data.frame = T) #alternativamente en la carpeta está el csv, entonces pueden usar ese si no les funciona
#adaptado del siguiente codigo
#https://stackoverflow.com/questions/56026550/how-to-use-hunspell-package-to-suggest-correct-words-in-a-column-in-r
cleantext = function(x) {
sapply(1:length(x), function(y) {
bad = hunspell(x[y], dict = costa)[[1]]
good = unlist(lapply(hunspell_suggest(bad,dict = costa), `[[`, 1))
if (length(bad)) {
for (i in 1:length(bad)) {
x[y] <<- gsub(bad[i], good[i], x[y])
}
}
})
x
}
hunspell::dictionary("costa/es_cr.dic")
#ya no tenemos el problema de los signos de exclamación a la mitad. Entonces no nos vamos a preocupar por esto
procesador_texto= function(x) {
otro  = str_trim(x) #le quitamos los espacios de mas
otro = otro[otro != ""] #quitamos las variables vacias basicamente
otro1 = cleantext(otro)
#View(cbind(otro,otro1)) #por si quieren comparar los resultados
ana = data.frame(texto = otro1, id = 1:length(otro1)) #si quisieramos podriamos conservar el id original del cuestionario. Para esto seleccionariamos las dos columnas y luego le haríamos un mutate o algo similar
# Stopwords--------------------
gato = ana %>%
unnest_tokens(word, texto) %>%  #en este paso separamos las palabras individualmente
anti_join(custom_stop)   #elimina las palabras que coincidan dentro del registro de stopwords
#vamos a agregar como una columna adicional las palabras lematizadas
z = hunspell_stem(words = gato$word,dict = costa) #lematizacion
vacios = which(lapply(z, length) == 0)
gato = gato[-vacios,] #las palabras divididas le quito los terminos no reconocidos #en este caso un numero
z = z[-vacios]
gato$lema = lapply(z, `[[`, 1) %>% as.character()
gato
}
ala = procesador_texto(dataset$PR10B)
frec_simple = ala %>% #frecuencias de las palabras lematizadas
count(lema, sort = T)
frec_lema = ala %>%
count(word,sort = T )
frec_simple15 = frec_simple[1:15,]
frec_simple_lema15 = frec_lema[1:15,]
#este grafico sirve para las palabras que no queremos lematizar. Como los nombres propios.
library(ggplot2)
grafico_palabras = function(a,titulo,subtitulo){
ggplot(data = a,aes(x= reorder(word,n) ,y =n))+
geom_point(color="blue")+
geom_segment(
aes(x=word,xend=word,y=0,yend=n),color="skyblue3")+
theme(axis.text.y = element_text(angle = 30,size=8,hjust=1))+
coord_flip()+
xlab("")+
theme_light()+
labs(title = titulo,
subtitle = subtitulo,
y = "Cantidad")+
theme(plot.title = element_text( hjust = 0.5),
plot.subtitle = element_text( hjust = 0.5),
panel.grid.major.y = element_blank(),
panel.border = element_blank(),
axis.ticks.y = element_blank())
}
#asi podemos aplicarlo más fácil para varias frecuencias de palabras
grafico_palabras(frec_simple15,"Temas principales en las respuestas de los participantes","Grafico de frecuencia por palabra, sin lematizar")
#asi podemos aplicarlo más fácil para varias frecuencias de palabras
grafico_palabras(frec_simple15,"Temas principales en las respuestas de los participantes","Grafico de frecuencia por palabra, sin lematizar")
#asi podemos aplicarlo más fácil para varias frecuencias de palabras
grafico_palabras(ala,,"Temas principales en las respuestas de los participantes","Grafico de frecuencia por palabra, sin lematizar")
#asi podemos aplicarlo más fácil para varias frecuencias de palabras
grafico_palabras(ala,word,"Temas principales en las respuestas de los participantes","Grafico de frecuencia por palabra, sin lematizar")
View(frec_simple_lema15)
View(frec_simple)
#asi podemos aplicarlo más fácil para varias frecuencias de palabras
grafico_palabras(frec_simple15,"word","Temas principales en las respuestas de los participantes","Grafico de frecuencia por palabra, sin lematizar")
#asi podemos aplicarlo más fácil para varias frecuencias de palabras
grafico_palabras(frec_simple15,frec_simple15$word,"Temas principales en las respuestas de los participantes","Grafico de frecuencia por palabra, sin lematizar")
View(frec_simple_lema15)
View(frec_simple15)
frec_simple = ala %>% #frecuencias de las palabras lematizadas
count(word, sort = T)
View(frec_simple)
frec_lema = ala %>%
count(lema,sort = T )
View(frec_lema)
colnames(frec_lema)
colnames(frec_lema) = c("word","n")
#asi podemos aplicarlo más fácil para varias frecuencias de palabras
grafico_palabras(frec_simple15,"Temas principales en las respuestas de los participantes","Grafico de frecuencia por palabra, sin lematizar")
#asi podemos aplicarlo más fácil para varias frecuencias de palabras
grafico_palabras(frec_simple15,"Temas principales en las respuestas de los participantes","Grafico de frecuencia por palabra, sin lematizar")
frec_simple15
frec_simple15 = frec_simple[1:15,]
frec_simple_lema15 = frec_lema[1:15,]
#asi podemos aplicarlo más fácil para varias frecuencias de palabras
grafico_palabras(frec_simple15,"Temas principales en las respuestas de los participantes","Grafico de frecuencia por palabra, sin lematizar")
grafico_palabras(frec_lema15,"Temas principales en las respuestas de los participantes","Grafico de frecuencia por palabra, sin lematizar")
#asi podemos aplicarlo más fácil para varias frecuencias de palabras
grafico_palabras(frec_simple15,"Temas principales en las respuestas de los participantes","Grafico de frecuencia por palabra, sin lematizar")
frec_lema = ala %>%
count(lema,sort = T )
colnames(frec_lema) = c("word","n")
frec_simple_lema15 = frec_lema[1:15,]
frec_lema15 = frec_lema[1:15,]
View(frec_lema15)
grafico_palabras(frec_lema15,"Temas principales en las respuestas de los participantes","Grafico de frecuencia por palabra, sin lematizar")
library(dplyr)
library(tidytext)
library(hunspell)
library(readxl)
library(tidytext)
library(SnowballC)
library(stringr)
#setwd("~/GitHub/Analisis-de-textos")
hunspell::dictionary("costa/es_cr.dic")
#modifiquen esto a conveniencia, en mi git estan esos diccionarios. Fernando me preguntó por qué había tantos diccionarios. Es solo para probar
espa=dictionary("mex/es_MX.dic")
costa = dictionary("costa/es_cr.dic")
cast = dictionary("cast/es_ES.dic")
custom_stop= data_frame(word = tm::stopwords("spanish"), #stopwords
lexicon = "custom") #el nombre que le vamos a poner
dataset <- foreign::read.spss("Actualidades.sav",to.data.frame = T) #alternativamente en la carpeta está el csv, entonces pueden usar ese si no les funciona
#adaptado del siguiente codigo
#https://stackoverflow.com/questions/56026550/how-to-use-hunspell-package-to-suggest-correct-words-in-a-column-in-r
cleantext = function(x) {
sapply(1:length(x), function(y) {
bad = hunspell(x[y], dict = costa)[[1]]
good = unlist(lapply(hunspell_suggest(bad,dict = costa), `[[`, 1))
if (length(bad)) {
for (i in 1:length(bad)) {
x[y] <<- gsub(bad[i], good[i], x[y])
}
}
})
x
}
#ya no tenemos el problema de los signos de exclamación a la mitad. Entonces no nos vamos a preocupar por esto
procesador_texto= function(x) {
otro  = str_trim(x) #le quitamos los espacios de mas
otro = otro[otro != ""] #quitamos las variables vacias basicamente
otro1 = cleantext(otro)
#View(cbind(otro,otro1)) #por si quieren comparar los resultados
ana = data.frame(texto = otro1, id = 1:length(otro1)) #si quisieramos podriamos conservar el id original del cuestionario. Para esto seleccionariamos las dos columnas y luego le haríamos un mutate o algo similar
# Stopwords--------------------
gato = ana %>%
unnest_tokens(word, texto) %>%  #en este paso separamos las palabras individualmente
anti_join(custom_stop)   #elimina las palabras que coincidan dentro del registro de stopwords
#vamos a agregar como una columna adicional las palabras lematizadas
z = hunspell_stem(words = gato$word,dict = costa) #lematizacion
vacios = which(lapply(z, length) == 0)
gato = gato[-vacios,] #las palabras divididas le quito los terminos no reconocidos #en este caso un numero
z = z[-vacios]
gato$lema = lapply(z, `[[`, 1) %>% as.character()
gato
}
ala = procesador_texto(dataset$PR10B)
frec_simple = ala %>% #frecuencias de las palabras lematizadas
count(word, sort = T)
frec_lema = ala %>%
count(lema,sort = T )
colnames(frec_lema) = c("word","n")
frec_simple15 = frec_simple[1:15,]
frec_lema15 = frec_lema[1:15,]
#este grafico sirve para las palabras que no queremos lematizar. Como los nombres propios.
library(ggplot2)
grafico_palabras = function(a,titulo,subtitulo){
ggplot(data = a,aes(x= reorder(word,n) ,y =n))+
geom_point(color="blue")+
geom_segment(
aes(x=word,xend=word,y=0,yend=n),color="skyblue3")+
theme(axis.text.y = element_text(angle = 30,size=8,hjust=1))+
coord_flip()+
xlab("")+
theme_light()+
labs(title = titulo,
subtitle = subtitulo,
y = "Cantidad")+
theme(plot.title = element_text( hjust = 0.5),
plot.subtitle = element_text( hjust = 0.5),
panel.grid.major.y = element_blank(),
panel.border = element_blank(),
axis.ticks.y = element_blank())
}
#asi podemos aplicarlo más fácil para varias frecuencias de palabras
grafico_palabras(frec_simple15,"Temas principales en las respuestas de los participantes","Grafico de frecuencia por palabra, sin lematizar")
grafico_palabras(frec_lema15,"Temas principales en las respuestas de los participantes","Grafico de frecuencia por palabra, sin lematizar")
grafico_palabras(frec_simple15,"Temas principales en las respuestas de los participantes","Grafico de frecuencia por palabra, sin lematizar")
grafico_palabras(frec_lema15,"Temas principales en las respuestas de los participantes","Grafico de frecuencia por palabra, sin lematizar")
grafico_palabras(frec_lema15,"Temas principales en las respuestas de los participantes","Grafico de frecuencia por palabra, lematizado")
library(dplyr)
library(tidytext)
library(hunspell)
library(readxl)
library(tidytext)
library(SnowballC)
library(stringr)
library(ggplot2)
#setwd("~/GitHub/Analisis-de-textos")
hunspell::dictionary("costa/es_cr.dic")
#modifiquen esto a conveniencia, en mi git estan esos diccionarios. Fernando me preguntó por qué había tantos diccionarios. Es solo para probar
espa=dictionary("mex/es_MX.dic")
costa = dictionary("costa/es_cr.dic")
cast = dictionary("cast/es_ES.dic")
custom_stop= data_frame(word = tm::stopwords("spanish"), #stopwords
lexicon = "custom") #el nombre que le vamos a poner
dataset <- foreign::read.spss("Actualidades.sav",to.data.frame = T) #alternativamente en la carpeta está el csv, entonces pueden usar ese si no les funciona
#adaptado del siguiente codigo
#https://stackoverflow.com/questions/56026550/how-to-use-hunspell-package-to-suggest-correct-words-in-a-column-in-r
cleantext = function(x) {
sapply(1:length(x), function(y) {
bad = hunspell(x[y], dict = costa)[[1]]
good = unlist(lapply(hunspell_suggest(bad,dict = costa), `[[`, 1))
if (length(bad)) {
for (i in 1:length(bad)) {
x[y] <<- gsub(bad[i], good[i], x[y])
}
}
})
x
}
#ya no tenemos el problema de los signos de exclamación a la mitad. Entonces no nos vamos a preocupar por esto
procesador_texto= function(x) {
otro  = str_trim(x) #le quitamos los espacios de mas
otro = otro[otro != ""] #quitamos las variables vacias basicamente
otro1 = cleantext(otro)
#View(cbind(otro,otro1)) #por si quieren comparar los resultados
ana = data.frame(texto = otro1, id = 1:length(otro1)) #si quisieramos podriamos conservar el id original del cuestionario. Para esto seleccionariamos las dos columnas y luego le haríamos un mutate o algo similar
# Stopwords--------------------
gato = ana %>%
unnest_tokens(word, texto) %>%  #en este paso separamos las palabras individualmente
anti_join(custom_stop)   #elimina las palabras que coincidan dentro del registro de stopwords
#vamos a agregar como una columna adicional las palabras lematizadas
z = hunspell_stem(words = gato$word,dict = costa) #lematizacion
vacios = which(lapply(z, length) == 0)
gato = gato[-vacios,] #las palabras divididas le quito los terminos no reconocidos #en este caso un numero
z = z[-vacios]
gato$lema = lapply(z, `[[`, 1) %>% as.character()
gato
}
grafico_palabras = function(a,titulo,subtitulo){
ggplot(data = a,aes(x= reorder(word,n) ,y =n))+
geom_point(color="blue")+
geom_segment(
aes(x=word,xend=word,y=0,yend=n),color="skyblue3")+
theme(axis.text.y = element_text(angle = 30,size=8,hjust=1))+
coord_flip()+
xlab("")+
theme_light()+
labs(title = titulo,
subtitle = subtitulo,
y = "Cantidad")+
theme(plot.title = element_text( hjust = 0.5),
plot.subtitle = element_text( hjust = 0.5),
panel.grid.major.y = element_blank(),
panel.border = element_blank(),
axis.ticks.y = element_blank())
}
ala = procesador_texto(dataset$PR10B)
frec_simple = ala %>% #frecuencias de las palabras lematizadas
count(word, sort = T)
frec_lema = ala %>%
count(lema,sort = T )
colnames(frec_lema) = c("word","n")
frec_simple15 = frec_simple[1:15,]
frec_lema15 = frec_lema[1:15,]
grafico_palabras(frec_simple15,"Temas principales en las respuestas de los participantes","Grafico de frecuencia por palabra, sin lematizar")
grafico_palabras(frec_lema15,"Temas principales en las respuestas de los participantes","Grafico de frecuencia por palabra, lematizado")
