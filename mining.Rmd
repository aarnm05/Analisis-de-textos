---
title: "R Notebook"
output: html_notebook
---

#cargar y preparar datos
```{r}
library(dplyr)
library(tidytext)
library(hunspell)
library(readxl)
library(tidytext)
library(SnowballC)
library(stringr)
library(ggplot2)
library( tidyr)
library( tm )

setwd("~/GitHub/Analisis-de-textos")

hunspell::dictionary("costa/es_cr.dic")
#modifiquen esto a conveniencia, en mi git estan esos diccionarios. Fernando me preguntó por qué había tantos diccionarios. Es solo para probar
espa=dictionary("mex/es_MX.dic")
costa = dictionary("costa/es_cr.dic")
cast = dictionary("cast/es_ES.dic")

custom_stop= data_frame(word = tm::stopwords("spanish"), #stopwords
                                          lexicon = "custom") #el nombre que le vamos a poner

dataset <-  suppressWarnings( foreign::read.spss("Actualidades.sav",to.data.frame = T) )#alternativamente en la carpeta está el csv, entonces pueden usar ese si no les funciona

#adaptado del siguiente codigo 
#https://stackoverflow.com/questions/56026550/how-to-use-hunspell-package-to-suggest-correct-words-in-a-column-in-r

cleantext = function(x) {
  
  sapply(1:length(x), function(y) {
    bad = hunspell(x[y], dict = costa)[[1]]
    good = unlist(lapply(hunspell_suggest(bad,dict = costa), `[[`, 1))
    
    if (length(bad)) {
      for (i in 1:length(bad)) {
        x[y] <<- gsub(bad[i], good[i], x[y])
      }
    }
  })
  x
}
```

#Definimos nuestras funciones

```{r}
#ya no tenemos el problema de los signos de exclamación a la mitad. Entonces no nos vamos a preocupar por esto

procesador_texto= function(x) {
  otro  = str_trim(x) #le quitamos los espacios de mas
  otro = otro[otro != ""] #quitamos las variables vacias basicamente
  
  otro1 = cleantext(otro)
  #View(cbind(otro,otro1)) #por si quieren comparar los resultados
  
  ana = data.frame(texto = otro1, id = 1:length(otro1)) #si quisieramos podriamos conservar el id original del cuestionario. Para esto seleccionariamos las dos columnas y luego le haríamos un mutate o algo similar
  
  # Stopwords--------------------
  
  gato = ana %>%
    unnest_tokens(word, texto) %>%  #en este paso separamos las palabras individualmente
    anti_join(custom_stop)   #elimina las palabras que coincidan dentro del registro de stopwords
  
#vamos a agregar como una columna adicional las palabras lematizadas

  z = hunspell_stem(words = gato$word,dict = costa) #lematizacion
  vacios = which(lapply(z, length) == 0)
  gato = gato[-vacios,] #las palabras divididas le quito los terminos no reconocidos #en este caso un numero
  z = z[-vacios]
  gato$lema = lapply(z, `[[`, 1) %>% as.character()
  
  gato
}

grafico_palabras = function(a,titulo,subtitulo){
  ggplot(data = a,aes(x= reorder(word,n) ,y =n))+
  geom_point(color="blue")+
  geom_segment(
    aes(x=word,xend=word,y=0,yend=n),color="skyblue3")+
  theme(axis.text.y = element_text(angle = 30,size=8,hjust=1))+
  coord_flip()+
  xlab("")+
  theme_light()+
  labs(title = titulo,
  subtitle = subtitulo,
  y = "Cantidad")+
  theme(plot.title = element_text( hjust = 0.5),
        plot.subtitle = element_text( hjust = 0.5),
        panel.grid.major.y = element_blank(),
        panel.border = element_blank(),
        axis.ticks.y = element_blank())
}

```

Hago los gráficos aparte de la función porque para lematizar vamos a necesitar la lista de palabras

#grafico 1
```{r}
ala = procesador_texto(dataset$PR10B)

frec_simple = ala %>% #frecuencias de las palabras lematizadas
    count(word, sort = T)
  
frec_lema = ala %>% 
  count(lema,sort = T )

colnames(frec_lema) = c("word","n")

frec_simple15 = frec_simple[1:15,]
frec_lema15 = frec_lema[1:15,]

grafico_palabras(frec_simple15,"Temas principales en las respuestas de los participantes","Grafico de frecuencia por palabra, sin lematizar")

grafico_palabras(frec_lema15,"Temas principales en las respuestas de los participantes","Grafico de frecuencia por palabra, lematizado")

```


#Matriz palabra conteo

```{r}
matriz = frec_simple %>%  #me produce la matriz que uso felipe, por el momento no se necesita
  pivot_wider(id_cols = id,names_from= word,values_from= n)
```

dict = costa
dataset$PR10B
```{r fun para bigramas}
palabras = dataset$PR10B
corrector.ortografia <- function(datos_texto, diccionario) 
  {
  
  datos_corregidos <- data.frame(texto = datos_texto)
  datos_corregidos$texto <- as.character(datos_corregidos$texto)
  datos_corregidos$nuevo <- 0

    for (i in 1:dim(datos_corregidos)[1]) 
      {
  
      respuesta <- datos_corregidos[i,"texto"]
      incorrectas <- hunspell(respuesta, dict = diccionario) # Detectar palabras incorrectas
      buenas <- NULL
     
        if(length(unlist(incorrectas)) > 0) 
          { 
         
          for (k in 1:length(incorrectas[[1]])) 
            {
              ## hunspell_suggest retorna una lista con un vector de posibles palabras para hacer la corrección
              buenas.sugeridas <- hunspell_suggest(incorrectas[[1]][k], dict = diccionario)
              buenas[k] <- buenas.sugeridas[[1]][1] ## La primera es por lo general la más probable.
            } 
          
       names(buenas) <- unlist(incorrectas)
       datos_corregidos$nuevo[i] <- str_replace_all(respuesta, buenas)
       
          } 
            else 
                {
                                             
                 datos_corregidos$nuevo[i] <- datos_corregidos[i,"texto"]
                 
                }

      }
       
    return(datos_corregidos)
  
}
corrector.lematizador <- function(datos_corregidos, diccionario) 
  {
  
  datos_corregidos$nuevo <- as.character(datos_corregidos$nuevo)
  datos_corregidos$nuevo_stem <- 0

    for (i in 1:dim(datos_corregidos)[1]) 
      {

      respuesta <- datos_corregidos[i,"nuevo"]
      palabras <- hunspell_parse(respuesta, format = "latex") ## Parsear el texto
      proveniente <- hunspell_stem(unlist(palabras), dict = diccionario) ## Retorna una lista con la raíz  de cada palabra

      texto <- NULL
  
         if(length(unlist(proveniente)) > 0) 
           { 
  
           for (j in 1:length(proveniente)) 
             {
      
             palabra <- proveniente[[j]][1]
             texto <- paste(texto, palabra, sep = " ")
             }
    
          datos_corregidos$nuevo_stem[i] <- texto
           }
            else 
              {
         
          datos_corregidos$nuevo_stem[i] <- respuesta

              }

       }

    return(datos_corregidos)
}
limpiar.texto <- function(datos){ 
  
  ## Transformar el texto a corpus
  corpus <- VCorpus(VectorSource(datos$nuevo_stem))
  
  ## Remover los números
  corpus <- tm_map(corpus, removeNumbers)
  
  ## Eliminar signos de puntuación
  corpus <- tm_map(corpus, removePunctuation)
  
  ## Convertir a minúscula
  corpus <- tm_map(corpus, content_transformer(tolower))
  
  ## Eliminar espacios en blanco de más
  corpus <- tm_map(corpus, stripWhitespace)
  
  ## Establecer palabras vacías:
  my_stopwords <- setdiff(stopwords("spanish"), c("también", "tampoco"))
  
  ## Eliminar palabras vacías
  corpus <- tm_map(corpus, removeWords, my_stopwords)
  
  corpus <- tm_map(corpus, stemDocument, language="spanish")
  
  return(corpus)    
  
}
palabras_bi = corrector.ortografia( palabras, costa)
palabras_bi2 = corrector.lematizador( palabras_bi, costa )
balabras_bifinal = limpiar.texto( palabras_bi2)
```


```{r posibles cambios a los bigramas de tesis}
texto.df <- data.frame(respuesta = sapply(balabras_bifinal, 
                                          as.character), 
                       stringsAsFactors = F)
#crear bigramas
Our_bigram = texto.df %>%
  unnest_tokens(bigram, respuesta, token = "ngrams", n = 2)

#contar los bigramas
Our_bigram_count = Our_bigram %>%
  count(bigram, sort = TRUE)

#Separar bigramas para filtrarlos
bigramas_separados = Our_bigram %>% 
  separate( bigram, c("word1", "word2"), sep = " ")

#bigramas con la palabra "no"
bigramas_con_no = bigramas_separados %>%
  filter(word1 == "no") %>%
  count(word1, word2, sort = TRUE)
```

Pendiente:


+ Graficos (añadir más)
terminar Lematizacion (listo; emir)
Agregar patrones regex y reemplazos para no distinguir tildes  (listo, mediante corrector; emir)
probar el codigo con la base de prueba (listo)

+Bigramas
Añadir token "no" antes de expresiones de negación  falta
Falta hacer graficos para Our_bigram_count y bigramas_con_no


+Red de correlaciones
Si da tiempo armar una de esos

+pensar en mas stop words


















Bibliografia https://www.tidytextmining.com/ngrams.html