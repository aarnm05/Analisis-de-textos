---
title: "R Notebook"
output: html_notebook
---


```{r}
library(dplyr)
library(tidytext)
library(hunspell)
library(readxl)
library(tidytext)
library(SnowballC)

hunspell::list_dictionaries()

#modifiquen esto a conveniencia, en mi git estan esos diccionarios
espa=dictionary("C:/Users/emirr/Documents/Sortone/OneDrive/Git/Analisis-de-textos/mex/es_MX.dic")
costa = dictionary("C:/Users/emirr/Documents/Sortone/OneDrive/Git/Analisis-de-textos/costa/es_cr.dic")
cast = dictionary("C:/Users/emirr/Documents/Sortone/OneDrive/Git/Analisis-de-textos/cast/es_ES.dic")

dataset <- read_excel("dataset.xlsx") #mi dataset 

k = hunspell(dataset$P1COM,dict = costa) #prueba de como funciona hunspeell


#cleantext function as defined by boski
#https://stackoverflow.com/questions/56026550/how-to-use-hunspell-package-to-suggest-correct-words-in-a-column-in-r

cleantext = function(x) {
  
  sapply(1:length(x), function(y) {
    bad = hunspell(x[y], dict = costa)[[1]]
    good = unlist(lapply(hunspell_suggest(bad,dict = costa), `[[`, 1))
    
    if (length(bad)) {
      for (i in 1:length(bad)) {
        x[y] <<- gsub(bad[i], good[i], x[y])
      }
    }
  })
  x
}

cleantext(dataset$P1COM[2]) ; dataset$P1COM[2] #cambia deceos por deceso.

ana = data.frame(texto= sapply(X =dataset$P1COM[1:31],cleantext,USE.NAMES = F),
                 id = 1:31
                 ) #evaluacion por filas; es decir para cada variable. Me produce un vector del mismo tamaño del vector inicial con el texto limpio. Al mismo tiempo es un data.frame cuyo nombre de columna es word. Por convencion

```



```{r}

custom_stop= data_frame(word = tm::stopwords("spanish"), #podemos hacerlo así
                                          lexicon = "custom")

gato = ana %>% 
  unnest_tokens(word, texto) %>%  #en este paso separamos las palabras individualmente
  anti_join(custom_stop)   #elimina las palabras que coincidan dentro del registro de stopwords
  
#gato tiene las palabras sin lematizar separadas, gato sirve para casos donde no queremos lematizar

frec_simple = gato %>% #frecuencias de las palabras sin lematizar 
  group_by(id) %>% 
  count(word,sort = T) 

#aqui podria ir un poco de regex


#lematizacion 

lema = select(gato,"word") %>%  #necesita un poco de trabajo todavia
  SnowballC::wordStem(language = "spanish")

```

```{r}
matriz = frec_simple %>%  #me produce la matriz que uso felipe, por el momento no se necesita
  pivot_wider(id_cols = id,names_from= word,values_from= n)
```

Pendiente:

Graficos(Emir puede subir una muestra o enseñarles varios)
Tablas
terminar Lematizacion
Agregar patrones regex y reemplazos para no distinguir tildes
probar el codigo con la base de prueba