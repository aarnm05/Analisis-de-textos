---
title: "R Notebook"
output: html_notebook
---


```{r}
library(dplyr)
library(tidytext)
library(hunspell)
library(readxl)
library(tidytext)
library(SnowballC)
library(stringr)

hunspell::list_dictionaries()

#modifiquen esto a conveniencia, en mi git estan esos diccionarios
espa=dictionary("C:/Users/emirr/Documents/Sortone/OneDrive/Git/Analisis-de-textos/mex/es_MX.dic")
costa = dictionary("C:/Users/emirr/Documents/Sortone/OneDrive/Git/Analisis-de-textos/costa/es_cr.dic")
cast = dictionary("C:/Users/emirr/Documents/Sortone/OneDrive/Git/Analisis-de-textos/cast/es_ES.dic")

dataset <- foreign::read.spss("PILOTO.sav",to.data.frame = T) #mi dataset 

otro  = str_trim(dataset$FU6B) #le quitamos los espacios de mas 
otro = otro[otro!=""] #quitamos las variables vacias basicamente

otro #podemos ver que hay problemas por el !.

hunspell(otro,dict = costa) #prueba de como funciona hunspell

#adaptado del siguiente codigo 
#https://stackoverflow.com/questions/56026550/how-to-use-hunspell-package-to-suggest-correct-words-in-a-column-in-r

cleantext = function(x) {
  
  sapply(1:length(x), function(y) {
    bad = hunspell(x[y], dict = costa)[[1]]
    good = unlist(lapply(hunspell_suggest(bad,dict = costa), `[[`, 1))
    
    if (length(bad)) {
      for (i in 1:length(bad)) {
        x[y] <<- gsub(bad[i], good[i], x[y])
      }
    }
  })
  x
}
 

#plan de contingencia

#ejemplo
ar= str_replace("nataci!n","\\!",replacement = "e") #basicamente vamos a hacer esto, reemplazar el signo de exclamacion por una vocal
ar
cleantext(ar)
remove(ar)
#fin del ejemplo

#hacemos esto porque la funcion toma un signo de exclamacion como el fin de una oracion (de forma correcta segun el español); consecuentemente lo va a tomar como si fuera dos palabras.


otro1 = str_replace_all(otro,"\\!", replacement = "e") #puede que no les funcione dependiendo de su version de R. En ese caso tendrían que borrarle un backslash.

cleantext(otro1)

#fin de plan de contingencia




cleantext(dataset$P1COM[2]) ; dataset$P1COM[2] #cambia deceos por deceso.

ana = data.frame(texto= sapply(X =dataset$P1COM[1:31],cleantext,USE.NAMES = F),
                 id = 1:31
                 ) #evaluacion por filas; es decir para cada variable. Me produce un vector del mismo tamaño del vector inicial con el texto limpio. Al mismo tiempo es un data.frame cuyo nombre de columna es word. Por convencion

```



```{r}

custom_stop= data_frame(word = tm::stopwords("spanish"), #podemos hacerlo así
                                          lexicon = "custom")

gato = ana %>% 
  unnest_tokens(word, texto) %>%  #en este paso separamos las palabras individualmente
  anti_join(custom_stop)   #elimina las palabras que coincidan dentro del registro de stopwords
  
#gato tiene las palabras sin lematizar separadas, gato sirve para casos donde no queremos lematizar

frec_simple = gato %>% #frecuencias de las palabras sin lematizar 
  group_by(id) %>% 
  count(word,sort = T) 

#aqui podria ir un poco de regex


#lematizacion 

lema = select(gato,"word") %>%  #necesita un poco de trabajo todavia
  SnowballC::wordStem(language = "spanish")

```

```{r}
matriz = frec_simple %>%  #me produce la matriz que uso felipe, por el momento no se necesita
  pivot_wider(id_cols = id,names_from= word,values_from= n)
```

Pendiente:

Graficos(Emir puede subir una muestra o enseñarles varios)
Tablas
terminar Lematizacion
Agregar patrones regex y reemplazos para no distinguir tildes
probar el codigo con la base de prueba