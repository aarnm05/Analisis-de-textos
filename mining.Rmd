---
title: "R Notebook"
output:
  html_document:
    df_print: paged
---

#cargar y preparar datos
```{r}
library(dplyr)
library(tidytext)
library(hunspell)
library(tidytext)
library(stringr)
library(ggplot2)
library( tidyr)
library(ggraph)
library(igraph)
#setwd("~/GitHub/Analisis-de-textos")

hunspell::dictionary("costa/es_cr.dic")
espa=dictionary("mex/es_MX.dic")
costa = dictionary("costa/es_cr.dic")
cast = dictionary("cast/es_ES.dic")

custom_stop= data_frame(word = tm::stopwords("spanish"), #stopwords
                                          lexicon = "custom") #el nombre que le vamos a poner
custom_stop = custom_stop[custom_stop$word!="no",]

dataset <-  suppressWarnings( foreign::read.spss("Actualidades.sav",to.data.frame = T) )

#adaptado del siguiente codigo 
#https://stackoverflow.com/questions/56026550/how-to-use-hunspell-package-to-suggest-correct-words-in-a-column-in-r

cleantext = function(x) {
  
  sapply(1:length(x), function(y) {
    bad = hunspell(x[y], dict = costa)[[1]]
    good = unlist(lapply(hunspell_suggest(bad,dict = costa), `[[`, 1))
    
    if (length(bad)) {
      for (i in 1:length(bad)) {
        x[y] <<- gsub(bad[i], good[i], x[y])
      }
    }
  })
  x
}
```

#Definimos nuestras funciones

```{r}
procesador_texto= function(x) {
  otro  = str_trim(x) #le quitamos los espacios de mas
  otro = otro[otro != ""] #quitamos las variables vacias basicamente
  otro1 = cleantext(otro)
  
  dummy = data.frame(texto = otro1, id = 1:length(otro1)) 
  # Stopwords--------------------
  
  beta = dummy %>%
    unnest_tokens(word, texto) %>%  #en este paso separamos las palabras individualmente
    anti_join(custom_stop)   #elimina las palabras que coincidan dentro del registro de stopwords
  
  z = hunspell_stem(words = beta$word,dict = costa) #lematizacion
  vacios = which(lapply(z, length) == 0)
  beta = beta[-vacios,] #las palabras divididas le quito los terminos no reconocidos #en este caso un numero
  z = z[-vacios]
  beta$lema = lapply(z, `[[`, 1) %>% as.character()
  
  beta
}

grafico_palabras = function(a,titulo,subtitulo){
  ggplot(data = a,aes(x = reorder(word,n) ,y = n))+
  geom_point(color = "blue")+
  geom_segment(
    aes(x=word,xend=word,y=0,yend=n),color="skyblue3")+
  theme(axis.text.y = element_text(angle = 30,size=8,hjust=1))+
  coord_flip()+
  xlab("")+
  theme_light()+
  labs(title = titulo,
  subtitle = subtitulo,
  y = "Cantidad")+
  theme(plot.title = element_text( hjust = 0.5),
        plot.subtitle = element_text( hjust = 0.5),
        panel.grid.major.y = element_blank(),
        panel.border = element_blank(),
        axis.ticks.y = element_blank())
}

```

Hago los gráficos aparte de la función porque para lematizar vamos a necesitar la lista de palabras

#grafico 1
```{r}

texto_procesado = procesador_texto(dataset$PR10B)

frec_lema = texto_procesado %>% 
  count(lema,sort = T )

colnames(frec_lema) = c("word","n")

frec_lema15 = frec_lema[1:15,]
grafico_palabras(frec_lema15,"Temas principales en las respuestas de los participantes","Grafico de frecuencia por palabra, lematizado")

```


#Matriz palabra conteo

```{r}
lema_bigrama = aggregate(lema~id, data = texto_procesado,paste, collapse=" ") #oraciones sin stopwords y con gramatica corregida

Our_bigram = lema_bigrama %>%
  select(lema) %>% 
  unnest_tokens(bigram, lema, token = "ngrams", n = 2)

#contar los bigramas
Our_bigram_count = Our_bigram %>%
  count(bigram, sort = TRUE)

#Separar bigramas para filtrarlos
bigramas_separados = Our_bigram_count %>% 
  separate( bigram, c("word1", "word2"), sep = " ")

bigrama_graf = bigramas_separados %>% 
  filter( n > 5) %>%   #un n deseado
  graph_from_data_frame()

#bigramas con la palabra "no"
bigramas_con_no = bigramas_separados %>%
  filter(word1 == "no") %>%
  count(word1, word2, sort = TRUE)

word_cors <- lema_bigrama %>%
  mutate(section = row_number()) %>%
  unnest_tokens(word, lema, token = "ngrams", n = 1)  %>%
  group_by(word) %>%
  filter(n() >= 5) %>%
  widyr::pairwise_cor(word, section,
                      sort = TRUE)

```


```{r graficos bigramas}

ggraph(bigrama_graf, layout = "fr") +
  geom_edge_link() +
  geom_node_point() +
  geom_node_text(aes(label = name), vjust = 1, hjust = 1)

word_cors %>%
  filter(correlation >= .25) %>%
  graph_from_data_frame() %>%
  ggraph(layout = "fr") +
  geom_edge_link(aes(#edge_alpha = correlation, 
                     edge_width = correlation, 
                     colour = correlation)) +
  geom_node_point(color = "#29e05d7a", size = 5) +
  geom_node_text(aes(label = name), repel = TRUE) +
  theme_void()

word_cors %>%
  filter( abs(correlation ) >= .25) %>%
  graph_from_data_frame() %>%
  ggraph(layout = "fr") +
  geom_edge_link(aes(#edge_alpha = correlation, 
                     edge_width = correlation, 
                     colour = correlation)) +
  geom_node_point(color = "#29e05d7a", size = 5) +
  geom_node_text(aes(label = name), repel = TRUE) +
  theme_void()

graf_bigrama = Our_bigram_count[1:15,]
colnames(graf_bigrama) = c("word","n")
grafico_palabras(a = graf_bigrama,
                 titulo = "Temas principales en las respuestas de los participantes",
                 subtitulo = "Grafico de frecuencia por palabra, con bigramas")

```

Preguntar al profe: pase las correlaciones a valor absoluto para quitar las bajas correlacion


Bibliografia:
https://www.tidytextmining.com/ngrams.html
citar la tesis de felipe